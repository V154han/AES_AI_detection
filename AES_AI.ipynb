{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "AES dataset: https://www.kaggle.com/competitions/asap-aes\n"
      ],
      "metadata": {
        "id": "D3fGeJ1qLFXh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "AI detection dataset: https://www.kaggle.com/datasets/thedrcat/daigt-v2-train-dataset"
      ],
      "metadata": {
        "id": "5m9UJMwaLZTg"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KA7kd74ZNjca"
      },
      "source": [
        "# Mounting Drive\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vaB8JVaFMc0N"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cOfgTQx5EqTV"
      },
      "source": [
        "# Pip installing gensim pyspellchecker and optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JXcrzPEcwMoO"
      },
      "outputs": [],
      "source": [
        "!pip install gensim pyspellchecker optuna"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvfxWyfxNtBd"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FXLZh6s9N9Sj"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    EarlyStoppingCallback,\n",
        "    AutoModel\n",
        ")\n",
        "from sklearn.metrics import r2_score\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from datasets import DatasetDict, Dataset\n",
        "import torch\n",
        "from torch.utils.data import Dataset, TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from torch import nn\n",
        "import re\n",
        "import optuna\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DYBBMhlSTTg"
      },
      "source": [
        "# Reading in data and performing EDA for the AES system"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AoHKenpId41s"
      },
      "source": [
        "### Reading in the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s7fZMPdhQjWH"
      },
      "outputs": [],
      "source": [
        "full_data = pd.read_excel(r'/content/drive/MyDrive/ASAP/training_set_rel3.xlsx')\n",
        "display(full_data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIgm8yscFOQ6"
      },
      "source": [
        "### Essay distribution counts for prompt 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JbwUMzJqFoCC"
      },
      "outputs": [],
      "source": [
        "prompt_8_isolated = full_data[full_data['essay_set'] == 8].copy()\n",
        "\n",
        "bins = [0, 10, 20, 30, 40, 50, 60]\n",
        "score_rng = ['0-10', '11-20', '21-30', '31-40', '41-50', '51-60']\n",
        "\n",
        "prompt_8_isolated['score_rng'] = pd.cut(prompt_8_isolated['domain1_score'], bins=bins, labels=score_rng, right = False)\n",
        "counts = prompt_8_isolated.groupby('score_rng').size()\n",
        "counts = pd.Series(counts)\n",
        "counts = counts.reset_index()\n",
        "counts = counts.rename(columns={0 : 'counts'})\n",
        "display(counts)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(counts['score_rng'], counts['counts'])\n",
        "plt.xlabel('Score Range')\n",
        "plt.ylabel('Number of Essays')\n",
        "plt.title('Number of Essays in Each Score Range for Prompt 8')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8OLpEhMeCUd"
      },
      "source": [
        "### Finding the number of essays in each prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AMpuzsAuY-Nx"
      },
      "outputs": [],
      "source": [
        "grouped_data = full_data.groupby('essay_set').count().reset_index()\n",
        "grouped_data_filtered = grouped_data[['essay_set', 'essay_id']]\n",
        "grouped_data_filtered.rename(columns={'essay_id': 'essay_count'}, inplace=True)\n",
        "display(grouped_data_filtered)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(grouped_data_filtered['essay_set'], grouped_data_filtered['essay_count'])\n",
        "plt.xlabel('Prompt Number')\n",
        "plt.ylabel('Number of Essays')\n",
        "plt.title('Number of Essays in Each Set')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ze3fs-XPHsgT"
      },
      "source": [
        "# Selecting the prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cSp2ucDA4L4f"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "prompt_data = full_data[full_data['essay_set'] == 4]\n",
        "prompt_data = prompt_data[~prompt_data['domain1_score'].isna()]\n",
        "prompt_data_train = prompt_data['essay'].astype(str)\n",
        "prompt_data_label = prompt_data['domain1_score'].astype(int)\n",
        "\n",
        "binned_labels = pd.qcut(prompt_data_label, q=5, duplicates='drop', labels = False)\n",
        "X_train_all, X_test_all, y_train_all, y_test_all = train_test_split(prompt_data_train, prompt_data_label, test_size=0.2, random_state=42, stratify = binned_labels)\n",
        "\n",
        "X_train_all = pd.Series(X_train_all)\n",
        "X_test_all = pd.Series(X_test_all)\n",
        "y_train_all = pd.Series(y_train_all)\n",
        "y_test_all = pd.Series(y_test_all)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCTKXqzqeHqG"
      },
      "source": [
        "# Investigating base-learner individual performances for the regression models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvWEUMg0eA46"
      },
      "source": [
        "### Creatng BERT-based model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ufqDlvMdGDFr"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "\n",
        "\n",
        "# Prompt 1 Best Params:  {'lr': 5e-05, 'batch_size': 8}\n",
        "# Prompt 7 Best Params: {'lr': 3e-05, 'batch_size': 8}\n",
        "# Prompt 8 Best Params: {'lr': 5e-05, 'batch_size': 8}\n",
        "def generate_stacking_preds(model_name, X_train, X_test, y_train, y_test, lr = 0.00001, bs = 16, epochs = 30):\n",
        "  class EssayDatasetRegression(Dataset):\n",
        "            def __init__(self, encodings, labels):\n",
        "                self.encodings = encodings\n",
        "                self.labels = labels\n",
        "\n",
        "            def __getitem__(self, idx):\n",
        "                item = {key: val[idx].clone().detach() for key, val in self.encodings.items()}\n",
        "                item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float)\n",
        "                return item\n",
        "\n",
        "            def __len__(self):\n",
        "                return len(self.labels)\n",
        "\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "\n",
        "  X_train = X_train.reset_index(drop=True)\n",
        "  y_train = y_train.reset_index(drop=True)\n",
        "\n",
        "  X_test = X_test.reset_index(drop=True)\n",
        "  y_test = y_test.reset_index(drop=True)\n",
        "\n",
        "  min_score = min(min(y_train), min(y_test))\n",
        "  max_score = max(max(y_train), max(y_test))\n",
        "\n",
        "  def make_compute_metrics(min_score, max_score):\n",
        "    def compute_metrics(eval_pred):\n",
        "          preds, labels = eval_pred\n",
        "          preds_rounded = np.clip(np.rint(preds), min_score, max_score).astype(int)\n",
        "          labels_rounded = np.rint(labels).astype(int)\n",
        "          return {'eval_qwk': cohen_kappa_score(preds_rounded, labels_rounded, weights='quadratic')}\n",
        "    return compute_metrics\n",
        "\n",
        "\n",
        "  def tokenize_texts(text):\n",
        "        cleaned_texts = []\n",
        "        for t in text:\n",
        "          t = t.replace('\\n', ' ').replace('\\r', ' ').replace('\\t', ' ')\n",
        "          t = re.sub(r'\\s+', ' ', t)\n",
        "          t = t.strip()\n",
        "          cleaned_texts.append(t)\n",
        "        return tokenizer(cleaned_texts, padding='max_length', truncation=True, max_length=512, return_tensors='pt')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  bined_y = pd.qcut(y_train, q=5, duplicates='drop', labels = False)\n",
        "  X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(X_train, y_train, test_size=0.2, random_state=42, stratify=bined_y)\n",
        "\n",
        "  X_train_split = X_train_split.tolist()\n",
        "  X_val_split = X_val_split.tolist()\n",
        "  y_train_split = y_train_split.tolist()\n",
        "  y_val_split = y_val_split.tolist()\n",
        "  X_test = X_test.tolist()\n",
        "  y_test = y_test.tolist()\n",
        "\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "  X_train_split_encoding = tokenize_texts(X_train_split)\n",
        "  X_test_encoding = tokenize_texts(X_test)\n",
        "  X_val_split_encoding = tokenize_texts(X_val_split)\n",
        "\n",
        "\n",
        "  X_train_split_dataset = EssayDatasetRegression(X_train_split_encoding, y_train_split)\n",
        "  X_test_dataset = EssayDatasetRegression(X_test_encoding, y_test)\n",
        "  X_val_split_dataset = EssayDatasetRegression(X_val_split_encoding, y_val_split)\n",
        "  # true_labels = y_test.tolist()\n",
        "\n",
        "  model2 = AutoModelForSequenceClassification.from_pretrained(model_name, problem_type = 'regression', num_labels=1)\n",
        "\n",
        "  for name, param in model2.named_parameters():\n",
        "      param.requires_grad = False\n",
        "\n",
        "  for name, param in model2.named_parameters():\n",
        "      if any(layer in name for layer in ['encoder.layer.8', 'encoder.layer.9','encoder.layer.10', 'encoder.layer.11', 'pooler', 'classifier']):\n",
        "          param.requires_grad = True\n",
        "\n",
        "\n",
        "\n",
        "  training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    learning_rate=lr,\n",
        "    per_device_train_batch_size=bs,\n",
        "    per_device_eval_batch_size=bs,\n",
        "    num_train_epochs= epochs,\n",
        "    logging_strategy = 'epoch',\n",
        "    eval_strategy = 'epoch',\n",
        "    save_strategy = 'epoch',\n",
        "    load_best_model_at_end = True,\n",
        "    metric_for_best_model = 'eval_qwk',\n",
        "    greater_is_better=True,\n",
        "    report_to = 'none',\n",
        "\n",
        "\n",
        "  )\n",
        "\n",
        "\n",
        "  trainer2 = Trainer(\n",
        "      model=model2,\n",
        "      args=training_args,\n",
        "      train_dataset=X_train_split_dataset,\n",
        "      compute_metrics = make_compute_metrics(min_score,max_score),\n",
        "      eval_dataset=X_val_split_dataset,\n",
        "      callbacks = [EarlyStoppingCallback(early_stopping_patience=5)]\n",
        "  )\n",
        "\n",
        "  trainer2.train()\n",
        "\n",
        "  results = trainer2.predict(X_test_dataset)\n",
        "  preds_test = np.rint(results.predictions.squeeze()).astype(int)\n",
        "\n",
        "  qwk = cohen_kappa_score(preds_test, y_test, weights='quadratic')\n",
        "  print(f'QWK: {qwk}')\n",
        "\n",
        "\n",
        "\n",
        "  ### This code below is for hyper-parameter tuning\n",
        "\n",
        "\n",
        "def generate_stacking_preds_hp_tune(model_name, X_train, X_test, y_train, y_test, lr = 0.00001, bs = 16, epochs = 30):\n",
        "  class EssayDatasetRegression(Dataset):\n",
        "            def __init__(self, encodings, labels):\n",
        "                self.encodings = encodings\n",
        "                self.labels = labels\n",
        "\n",
        "            def __getitem__(self, idx):\n",
        "                item = {key: val[idx].clone().detach() for key, val in self.encodings.items()}\n",
        "                item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float)\n",
        "                return item\n",
        "\n",
        "            def __len__(self):\n",
        "                return len(self.labels)\n",
        "\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "\n",
        "  X_train = X_train.reset_index(drop=True)\n",
        "  y_train = y_train.reset_index(drop=True)\n",
        "\n",
        "  X_test = X_test.reset_index(drop=True)\n",
        "  y_test = y_test.reset_index(drop=True)\n",
        "\n",
        "  min_score = min(min(y_train), min(y_test))\n",
        "  max_score = max(max(y_train), max(y_test))\n",
        "\n",
        "  def make_compute_metrics(min_score, max_score):\n",
        "    def compute_metrics(eval_pred):\n",
        "          preds, labels = eval_pred\n",
        "          preds_rounded = np.clip(np.rint(preds), min_score, max_score).astype(int)\n",
        "          labels_rounded = np.rint(labels).astype(int)\n",
        "          return {'eval_qwk': cohen_kappa_score(preds_rounded, labels_rounded, weights='quadratic')}\n",
        "    return compute_metrics\n",
        "\n",
        "  def tokenize_texts(text):\n",
        "          cleaned_texts = []\n",
        "          for t in text:\n",
        "            t = t.replace('\\n', ' ').replace('\\r', ' ').replace('\\t', ' ')\n",
        "            t = re.sub(r'\\s+', ' ', t)\n",
        "            t = t.strip()\n",
        "            cleaned_texts.append(t)\n",
        "          return tokenizer(cleaned_texts, padding='max_length', truncation=True, max_length=512, return_tensors='pt')\n",
        "\n",
        "\n",
        "\n",
        "  model = AutoModelForSequenceClassification.from_pretrained(model_name, problem_type = 'regression', num_labels=1)\n",
        "\n",
        "  binned_y = pd.qcut(y_train, q=5, duplicates='drop', labels = False)\n",
        "\n",
        "  X_train_ft, X_val, y_train_ft, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42, stratify= binned_y)\n",
        "\n",
        "\n",
        "  train_data = X_train_ft.tolist()\n",
        "  val_data = X_val.tolist()\n",
        "  train_labels = y_train_ft.tolist()\n",
        "  val_labels = y_val.tolist()\n",
        "\n",
        "  train_encoding = tokenize_texts(train_data)\n",
        "  val_encoding = tokenize_texts(val_data)\n",
        "\n",
        "\n",
        "  train_dataset = EssayDatasetRegression(train_encoding, train_labels)\n",
        "  val_dataset = EssayDatasetRegression(val_encoding, val_labels)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  for name, param in model.named_parameters():\n",
        "      param.requires_grad = False\n",
        "\n",
        "  for name, param in model.named_parameters():\n",
        "    if any(layer in name for layer in ['encoder.layer.8', 'encoder.layer.9','encoder.layer.10', 'encoder.layer.11', 'pooler', 'classifier']):\n",
        "        param.requires_grad = True\n",
        "\n",
        "        # min_score = min(y_train)\n",
        "        # max_score = max(y_train)\n",
        "\n",
        "\n",
        "  training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    learning_rate=lr,\n",
        "    per_device_train_batch_size=bs,\n",
        "    per_device_eval_batch_size=bs,\n",
        "    num_train_epochs= epochs,\n",
        "    logging_strategy = 'epoch',\n",
        "    eval_strategy = 'epoch',\n",
        "    save_strategy = 'epoch',\n",
        "    load_best_model_at_end = True,\n",
        "    metric_for_best_model = 'eval_loss',\n",
        "    greater_is_better=False,\n",
        "    report_to = 'none',\n",
        "\n",
        "\n",
        "  )\n",
        "\n",
        "\n",
        "\n",
        "  trainer = Trainer(\n",
        "      model=model,\n",
        "      args=training_args,\n",
        "      train_dataset=train_dataset,\n",
        "      eval_dataset=val_dataset,\n",
        "      compute_metrics=make_compute_metrics(min_score, max_score),\n",
        "      callbacks = [EarlyStoppingCallback(early_stopping_patience=3)]\n",
        "  )\n",
        "\n",
        "  trainer.train()\n",
        "\n",
        "  results = trainer.predict(val_dataset)\n",
        "  preds_val = np.rint(results.predictions.squeeze()).astype(int)\n",
        "\n",
        "  qwk_val = cohen_kappa_score(preds_val, y_val, weights='quadratic')\n",
        "  print(f'QWK: {qwk_val}')\n",
        "\n",
        "  return qwk_val\n",
        "\n",
        "def tune_hyperparameters(trial):\n",
        "    lr = trial.suggest_float('lr', 1e-5, 5e-5, step=1e-5)\n",
        "    bs = trial.suggest_categorical('batch_size', [8, 16, 32])\n",
        "\n",
        "    print(f'Trial {trial.number}: lr={lr}, batch_size={bs}')\n",
        "\n",
        "\n",
        "\n",
        "    qwk  = generate_stacking_preds_hp_tune('bert-base-uncased', X_train_all, X_test_all, y_train_all, y_test_all, lr=lr, bs = bs)\n",
        "\n",
        "\n",
        "\n",
        "    return qwk\n",
        "\n",
        "\n",
        "# study = optuna.create_study(direction='maximize')\n",
        "# study.optimize(tune_hyperparameters, n_trials=10)\n",
        "# print('Number of finished trials:', len(study.trials))\n",
        "# print('Best trial:', study.best_trial.params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhtMnmwvJvDo"
      },
      "source": [
        "### Checking the run time and performance for the BERT-based model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mtOQa-wCmFW5"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "generate_stacking_preds('bert-base-uncased', X_train_all, X_test_all, y_train_all, y_test_all, lr = 0.00005, bs=8\n",
        "                                                                                                                , epochs=80)\n",
        "\n",
        "end_time = time.time()\n",
        "execution_time = end_time - start_time\n",
        "print(f'Execution time: {execution_time} seconds')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0bXqld3Lc1b"
      },
      "source": [
        "### Downloading the NLTK package"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9FRZfn8AlVsK"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import word_tokenize\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lizv31SL5cK"
      },
      "source": [
        "### Creating the LSTM-based model\n",
        "\n",
        "\n",
        "\n",
        "*   Although in my dissertation I said I used a similar structure to Taghipor's and Ng's model - this was based of their research paper rather than the code (which I could not find). No part of their actual code was used here.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sCydp1c8bUWl"
      },
      "outputs": [],
      "source": [
        "import gensim.downloader as api\n",
        "from gensim.models import KeyedVectors\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from collections import Counter\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "import optuna\n",
        "import time\n",
        "import re\n",
        "# Prompt 1 optim params: {'hidden_dim': 256, 'lr': 0.00040147129315448076, 'weight_decay': 4.194966656572804e-05, 'dropout1': 0.48972751622541333, 'dropout2': 0.5637289114635329, 'dropout3': 0.4006279296851355, 'batch_size': 64}\n",
        "# Prompt 2 Best Params: {'hidden_dim': 128, 'lr': 0.0008329626514872093, 'weight_decay': 1.10904162552094e-06, 'dropout1': 0.4113129372112616, 'dropout2': 0.5339519738927687, 'dropout3': 0.6474745827693765, 'batch_size': 32}\n",
        "# Prompt 3 Best Params: {'hidden_dim': 64, 'lr': 0.0009176211868710996, 'weight_decay': 8.47468900298905e-05, 'dropout1': 0.4402196829382994, 'dropout2': 0.6543812752462479, 'dropout3': 0.30572393075878385, 'batch_size': 128}\n",
        "# Prompt 4 Best Params: Best trial: {'hidden_dim': 512, 'lr': 0.0006392739979334983, 'weight_decay': 1.0246979767749631e-06, 'dropout1': 0.6331948007619779, 'dropout2': 0.3024115352348601, 'dropout3': 0.4826430525213473, 'batch_size': 32}\n",
        "# Prompt 5 Best Params: {'hidden_dim': 256, 'lr': 0.0007888666643834901, 'weight_decay': 1.9973208515938436e-05, 'dropout1': 0.5150492631843749, 'dropout2': 0.6836007071470228, 'dropout3': 0.44931934750457436, 'batch_size': 16}\n",
        "# Prompt 6 Best Params: {'hidden_dim': 256, 'lr': 0.0004221715369392723, 'weight_decay': 1.6693933987910344e-06, 'dropout1': 0.642989336447883, 'dropout2': 0.6149595736926003, 'dropout3': 0.4561023529421714, 'batch_size': 16}\n",
        "# Prompt 7 Best Params: {'hidden_dim': 128, 'lr': 0.0007031295542079031, 'weight_decay': 1.1167323499868158e-05, 'dropout1': 0.43014203679075136, 'dropout2': 0.6072345773651249, 'dropout3': 0.5301384805467484, 'batch_size': 64}\n",
        "# Prompt 8 Best Params: {'hidden_dim': 256, 'lr': 0.00044054250769868446, 'weight_decay': 1.8209940215624457e-05, 'dropout1': 0.6820871115494689, 'dropout2': 0.5530971947983455, 'dropout3': 0.6417406365573428, 'batch_size': 64}\n",
        "def lstm_model(X_train, X_test, y_train, y_test):\n",
        "    w2v_model = api.load('word2vec-google-news-300')\n",
        "    embedding_size = w2v_model.vector_size\n",
        "\n",
        "    class Vocab:\n",
        "        def __init__(self, token_freqs, min_freq=1, specials=['<pad>', '<unk>']):\n",
        "            self.itos = list(specials)\n",
        "            self.stoi = {tok: i for i, tok in enumerate(self.itos)}\n",
        "            for token, freq in token_freqs.items():\n",
        "                if freq >= min_freq and token not in self.stoi:\n",
        "                    self.stoi[token] = len(self.itos)\n",
        "                    self.itos.append(token)\n",
        "        def __len__(self):\n",
        "            return len(self.itos)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    class EssayDataset(Dataset):\n",
        "        def __init__(self, essays, labels, max_len, vocab):\n",
        "            self.essays = essays\n",
        "            self.labels = labels\n",
        "            self.max_length = max_len\n",
        "            self.vocab = vocab\n",
        "\n",
        "        def create_encodings(self, text):\n",
        "            word_token = word_tokenize(text.lower())\n",
        "            input_ids = [self.vocab.stoi.get(word, self.vocab.stoi['<unk>']) for word in word_token]\n",
        "            if len(input_ids) < self.max_length:\n",
        "                input_ids += [self.vocab.stoi['<pad>']] * (self.max_length - len(input_ids))\n",
        "            else:\n",
        "                input_ids = input_ids[:self.max_length]\n",
        "            return torch.tensor(input_ids)\n",
        "\n",
        "        def __getitem__(self, index):\n",
        "            essay = self.create_encodings(self.essays[index])\n",
        "            label = self.labels[index]\n",
        "            return essay, label\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self.essays)\n",
        "\n",
        "    class BiLSTM_CNN(nn.Module):\n",
        "        def __init__(self, vocab_size, embedding_dim, hidden_dim, embedding_matrix):\n",
        "            super().__init__()\n",
        "            self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
        "            self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
        "            self.conv1 = nn.Conv1d(hidden_dim * 2, hidden_dim, kernel_size=3, padding=1)\n",
        "            self.dropout1 = nn.Dropout( 0.6820871115494689)\n",
        "            self.conv2 = nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, padding=1)\n",
        "            self.dropout2 = nn.Dropout(0.5530971947983455)\n",
        "            self.pool = nn.AdaptiveAvgPool1d(1)\n",
        "            self.dropout3 = nn.Dropout(0.6417406365573428)\n",
        "            self.fc = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "        def forward(self, x):\n",
        "            embedded = self.embedding(x)\n",
        "            lstm_out, _ = self.lstm(embedded)\n",
        "            conv1_out = torch.relu(self.conv1(lstm_out.permute(0, 2, 1)))\n",
        "            conv2_out = torch.relu(self.conv2(conv1_out))\n",
        "            pool_out = self.pool(conv2_out).squeeze(2)\n",
        "            output = self.fc(pool_out)\n",
        "            return output.squeeze()\n",
        "\n",
        "    def cleaning_data(text):\n",
        "      text = text.lower()\n",
        "      text = text.replace('\\n', ' ')\n",
        "      text = text.replace('\\t', ' ')\n",
        "      text = text.replace('\\r', ' ')\n",
        "      text = re.sub(r'\\s+', ' ', text)\n",
        "      text = text.strip()\n",
        "      return text\n",
        "\n",
        "    essay_data = X_train.tolist() + X_test.tolist()\n",
        "    essays = [cleaning_data(essay) for essay in essay_data]\n",
        "    X_train = pd.Series(essays[:len(X_train)])\n",
        "    X_test = pd.Series(essays[len(X_train):])\n",
        "    counter = Counter()\n",
        "    for essay in essays:\n",
        "        counter.update(word_tokenize(essay.lower()))\n",
        "    most_common = counter.most_common(4000)\n",
        "    vocab = Vocab(dict(most_common), min_freq=1)\n",
        "\n",
        "    embedding_matrix = np.zeros((len(vocab), embedding_size))\n",
        "    for i, word in enumerate(vocab.itos):\n",
        "        embedding_matrix[i] = w2v_model[word] if word in w2v_model else np.random.normal(scale=0.6, size=(embedding_size,))\n",
        "    embedding_matrix = torch.tensor(embedding_matrix, dtype=torch.float32)\n",
        "\n",
        "    min_score = int(min(y_train.min(), y_test.min()))\n",
        "    max_score = int(max(y_train.max(), y_test.max()))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    bins = pd.qcut(y_train, q=5, duplicates='drop', labels = False)\n",
        "    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    oof_preds = np.zeros(len(y_train))\n",
        "    y_train_values = np.zeros(len(y_train))\n",
        "    test_preds = []\n",
        "    scaler = MinMaxScaler()\n",
        "    scaler.fit(y_train.values.reshape(-1, 1))\n",
        "\n",
        "    #Comment this out to do fine-tuning\n",
        "    # def tuning_parameeters(trial):\n",
        "    #   split = int(0.8 * len(X_train))\n",
        "    #   X_tr, X_val = X_train[:split], X_train[split:]\n",
        "    #   y_tr, y_val = y_train[:split], y_train[split:]\n",
        "\n",
        "    #   hidden_dim = trial.suggest_categorical('hidden_dim', [64, 128, 256, 512])\n",
        "    #   lr = trial.suggest_float('lr', 1e-4, 1e-3, step=1e-4)\n",
        "    #   weight_decay = trial.suggest_float('weight_decay', 1e-5, 1e-4, step = 1e-5)\n",
        "    #   dropout1 = trial.suggest_float('dropout1', 0.2, 0.8, step=0.1)\n",
        "    #   dropout2 = trial.suggest_float('dropout2', 0.2, 0.8, step=0.1)\n",
        "    #   dropout3 = trial.suggest_float('dropout3', 0.2, 0.8, step=0.1)\n",
        "    #   batch_size = trial.suggest_categorical('batch_size', [16, 32, 64, 128])\n",
        "\n",
        "\n",
        "    #   y_train_scaled_full = scaler.transform(y_tr.values.reshape(-1, 1)).flatten()\n",
        "    #   y_val_scaled = scaler.transform(y_val.values.reshape(-1, 1)).flatten()\n",
        "\n",
        "    #   full_train_dataset = EssayDataset(X_tr.tolist(), y_train_scaled_full, 512, vocab)\n",
        "    #   full_train_loader = DataLoader(full_train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    #   full_val_dataset = EssayDataset(X_val.tolist(), y_val_scaled, 512, vocab)\n",
        "    #   full_val_loader = DataLoader(full_val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    #   model = BiLSTM_CNN(len(vocab), embedding_size, hidden_dim, embedding_matrix)\n",
        "    #   model.dropout1.p = dropout1\n",
        "    #   model.dropout2.p = dropout2\n",
        "    #   model.dropout3.p = dropout3\n",
        "    #   optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    #   criterion = nn.MSELoss()\n",
        "\n",
        "    #   for epoch in range(5):\n",
        "    #       model.train()\n",
        "    #       total_loss = 0\n",
        "    #       for essays, labels in full_train_loader:\n",
        "    #           optimizer.zero_grad()\n",
        "    #           output = model(essays)\n",
        "    #           loss = criterion(output, labels.float())\n",
        "    #           total_loss += loss.item()\n",
        "    #           loss.backward()\n",
        "    #           optimizer.step()\n",
        "\n",
        "    #   model.eval()\n",
        "    #   val_preds = []\n",
        "    #   with torch.no_grad():\n",
        "    #       for essays, _ in full_val_loader:\n",
        "    #           output = model(essays).cpu().numpy()\n",
        "    #           val_preds.extend(output)\n",
        "    #   val_preds = scaler.inverse_transform(np.array(val_preds).reshape(-1, 1)).flatten()\n",
        "    #   val_preds_rounded = np.clip(np.rint(val_preds), min_score, max_score).astype(int)\n",
        "    #   qwk = cohen_kappa_score(val_preds_rounded, y_val.values, weights='quadratic')\n",
        "    #   return qwk\n",
        "\n",
        "    # study = optuna.create_study(direction='maximize')\n",
        "    # study.optimize(tuning_parameeters, n_trials=10)\n",
        "    # print('Number of finished trials:', len(study.trials))\n",
        "    # print('Best trial:', study.best_trial.params)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    y_train_scaled_full = scaler.transform(y_train.values.reshape(-1, 1)).flatten()\n",
        "\n",
        "    full_train_dataset = EssayDataset(X_train.tolist(), y_train_scaled_full, 512, vocab)\n",
        "    full_train_loader = DataLoader(full_train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "    model2 = BiLSTM_CNN(len(vocab), embedding_size, 256, embedding_matrix)\n",
        "    optimizer = torch.optim.Adam(model2.parameters(), lr = 0.00044054250769868446, weight_decay = 1.8209940215624457e-05)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    for epoch in range(10):\n",
        "        model2.train()\n",
        "        total_loss = 0\n",
        "        for essays, labels in full_train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            output = model2(essays)\n",
        "            loss = criterion(output, labels.float())\n",
        "            total_loss += loss.item()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        print(f'Epoch {epoch + 1}, Loss: {total_loss / len(full_train_loader)}')\n",
        "\n",
        "\n",
        "    model2.eval()\n",
        "    test_dataset = EssayDataset(X_test.tolist(), np.zeros(len(X_test)), 512, vocab)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "    final_test_preds = []\n",
        "    with torch.no_grad():\n",
        "        for essays, _ in test_loader:\n",
        "            output = model2(essays).cpu().numpy().flatten()\n",
        "            final_test_preds.extend(output)\n",
        "\n",
        "    final_test_preds = scaler.inverse_transform(np.array(final_test_preds).reshape(-1, 1)).flatten()\n",
        "    final_preds_rounded = np.clip(np.rint(final_test_preds), min_score, max_score).astype(int)\n",
        "\n",
        "    qwk = cohen_kappa_score(y_test.astype(int), final_preds_rounded, weights='quadratic')\n",
        "    print(f'QWK: {qwk}')\n",
        "\n",
        "\n",
        "    # return oof_preds, final_preds_rounded\n",
        "    # return oof_preds, final_preds_rounded, avg_test_preds, y_train_values, y_test\n",
        "\n",
        "\n",
        "\n",
        "# start_time = time.time()\n",
        "# _ = bert_lstm_model(X_train_all, X_test_all, y_train_all, y_test_all)\n",
        "# end_time = time.time()\n",
        "# execution_time = end_time - start_time\n",
        "# print(f'Execution time: {execution_time} seconds')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21ffIPxCez1A"
      },
      "source": [
        "### Checking the run time and performance for the LSTM-based model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CpvUju53u3lj"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "lstm_model(X_train_all, X_test_all, y_train_all, y_test_all)\n",
        "\n",
        "end_time = time.time()\n",
        "execution_time = end_time - start_time\n",
        "print(f'Execution time: {execution_time} seconds')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MtFzZCQux9VJ"
      },
      "source": [
        "### Creating the Random Forest model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "foMT-9QTFQ3x"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from sklearn.base import clone\n",
        "import spacy\n",
        "from spellchecker import SpellChecker\n",
        "\n",
        "# Prompt 1 Best Params: n_estimators=500, max_depth=10, random_state=42, n_jobs=-1, max_features = 'sqrt', min_samples_split = 2\n",
        "# Prompt 7 Best: Params: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_split': 5, 'n_estimators': 500}\n",
        "# Prompt 8 Best Params:  {'max_depth': 20, 'max_features': 'sqrt', 'min_samples_split': 5, 'n_estimators': 100}\n",
        "\n",
        "\n",
        "def random_forest_model_with_gridsearch(X_train, X_test, y_train, y_test):\n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "    spell = SpellChecker()\n",
        "\n",
        "    def cleaning_data(text):\n",
        "      text = text.replace('\\n', ' ')\n",
        "      text = text.replace('\\t', ' ')\n",
        "      text = text.replace('\\r', ' ')\n",
        "      text = re.sub(r'\\s+', ' ', text)\n",
        "      text = text.strip()\n",
        "      return text\n",
        "\n",
        "    X_train = X_train.apply(cleaning_data)\n",
        "    X_test = X_test.apply(cleaning_data)\n",
        "\n",
        "    def feature_creation(essay):\n",
        "        doc = nlp(essay)\n",
        "        num_sentences = len(list(doc.sents))\n",
        "        num_words = len(essay.split())\n",
        "        num_unique_words = len(set(essay.split()))\n",
        "        num_stop_words = len([token for token in doc if token.is_stop])\n",
        "        num_proper_nouns = len([token for token in doc if token.pos_ == 'PROPN'])\n",
        "        num_verbs = len([token for token in doc if token.pos_ == 'VERB'])\n",
        "        num_adjectives = len([token for token in doc if token.pos_ == 'ADJ'])\n",
        "        num_adverbs = len([token for token in doc if token.pos_ == 'ADV'])\n",
        "        num_nouns = len([token for token in doc if token.pos_ == 'NOUN'])\n",
        "        num_prepositions = len([token for token in doc if token.pos_ == 'ADP'])\n",
        "        num_pronouns = len([token for token in doc if token.pos_ == 'PRON'])\n",
        "        num_conjunctions = len([token for token in doc if token.pos_ == 'CCONJ'])\n",
        "        num_interjections = len([token for token in doc if token.pos_ == 'INTJ'])\n",
        "        num_punctuation = len([token for token in doc if token.pos_ == 'PUNCT'])\n",
        "        num_digits = len([token for token in doc if token.pos_ == 'NUM'])\n",
        "        num_entities = len(list(doc.ents))\n",
        "        num_spelling_errors = len(spell.unknown(essay.split()))\n",
        "        avh_word_length = np.mean([len(word) for word in essay.split()])\n",
        "        avg_sentence_length = np.mean([len(sent) for sent in list(doc.sents)])\n",
        "        return [num_sentences, num_words, num_unique_words, num_stop_words, num_proper_nouns, num_verbs, num_adjectives, num_adverbs, num_nouns, num_prepositions, num_pronouns,\n",
        "                num_conjunctions, num_interjections, num_punctuation, num_digits, num_entities, num_spelling_errors, avh_word_length, avg_sentence_length]\n",
        "\n",
        "    features = np.array([feature_creation(essay) for essay in X_train])\n",
        "    features_test = np.array([feature_creation(essay) for essay in X_test])\n",
        "\n",
        "    min_score = min(y_train.min(), y_test.min())\n",
        "    max_score = max(y_train.max(), y_test.max())\n",
        "\n",
        "    binned_y = pd.qcut(y_train, q=5, duplicates='drop', labels=False)\n",
        "    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    oof_preds = np.zeros(len(y_train))\n",
        "    y_train_values = np.zeros(len(y_train))\n",
        "    y_preds = []\n",
        "\n",
        "    param_grid = {\n",
        "        'n_estimators': [100, 300, 500],\n",
        "        'max_depth': [None, 10, 20, 50],\n",
        "        'min_samples_split': [2, 5, 7],\n",
        "        'max_features': ['sqrt', 'log2']\n",
        "    }\n",
        "\n",
        "    grid_search = GridSearchCV(\n",
        "            RandomForestRegressor(random_state=42),\n",
        "            param_grid,\n",
        "            cv=3,\n",
        "            n_jobs=-1,\n",
        "            scoring='neg_mean_squared_error'\n",
        "        )\n",
        "    grid_search.fit(features, y_train)\n",
        "    print('Best Parameters:', grid_search.best_params_)\n",
        "\n",
        "    best_model = grid_search.best_estimator_\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    model = clone(best_model)\n",
        "    model.fit(features, y_train)\n",
        "\n",
        "\n",
        "    y_pred = model.predict(features_test)\n",
        "    y_pred_rounded = np.clip(np.rint(y_pred), min_score, max_score).astype(int)\n",
        "    qwk = cohen_kappa_score(y_test, y_pred_rounded, weights='quadratic')\n",
        "    print(f'QWK: {qwk}')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gaRb3G_NjbE"
      },
      "source": [
        "### Checking the run time and performance for the Random Forest model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2qEe9ngsd03e"
      },
      "outputs": [],
      "source": [
        "start_time = time.time()\n",
        "random_forest_model_with_gridsearch(X_train_all, X_test_all, y_train_all, y_test_all)\n",
        "end_time = time.time()\n",
        "execution_time = end_time - start_time\n",
        "print(f'Execution time: {execution_time} seconds')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKiHlhW8fNc5"
      },
      "source": [
        "# Investigating base-learner individual performance for classification models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrAlkWNyODgn"
      },
      "source": [
        "### Creaating the BERT-based model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UDCH7gF-z-g4"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "from sklearn.metrics import cohen_kappa_score, accuracy_score\n",
        "import optuna\n",
        "from torch.nn.functional import softmax\n",
        "import re\n",
        "\n",
        "\n",
        "# Prompt 1 Best Params:: {'lr': 4e-05, 'batch_size': 32}\n",
        "# Prompt 2 Best Params: {'lr': 4e-05, 'batch_size': 8}\n",
        "# Prompt 3 Best Params: {'lr': 3e-05, 'batch_size': 8}\n",
        "# Prompt 4 Best Params: {'lr': 4e-05, 'batch_size': 8}\n",
        "# Prompt 5 Best Params: {'lr': 4e-05, 'batch_size': 8}\n",
        "# Prompt 6 Best Params: {'lr': 5e-05, 'batch_size': 8}\n",
        "# Prompt 7 Best Params: {'lr': 3e-05, 'batch_size': 8}\n",
        "# Prompt 8 Best Params: {'lr': 5e-05, 'batch_size': 8}\n",
        "def generate_stacking_preds_classification(model_name, X_train, X_test, y_train, y_test, lr = 0.00001, bs = 16, epochs = 30):\n",
        "  class EssayDatasetClassification(Dataset):\n",
        "        def __init__(self, encodings, labels, seq_len = 512):\n",
        "            self.encodings = encodings\n",
        "            self.labels = labels\n",
        "            self.seq_len = 512\n",
        "\n",
        "        def __getitem__(self, item):\n",
        "            return {\n",
        "                'input_ids': self.encodings['input_ids'][item].clone().detach(),\n",
        "                'attention_mask': self.encodings['attention_mask'][item].clone().detach(),\n",
        "                'labels': torch.tensor(self.labels[item], dtype=torch.long)\n",
        "            }\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self.labels)\n",
        "\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "\n",
        "  X_train = X_train.reset_index(drop=True)\n",
        "  y_train = y_train.reset_index(drop=True)\n",
        "\n",
        "  X_test = X_test.reset_index(drop=True)\n",
        "  y_test = y_test.reset_index(drop=True)\n",
        "\n",
        "  unique_scores = sorted(list(set(np.concatenate([np.array(y_train), np.array(y_test)]))))\n",
        "  score_to_class = {score: i for i, score in enumerate(unique_scores)}\n",
        "  class_to_score = {i: score for score, i in score_to_class.items()}\n",
        "  num_classes = len(score_to_class)\n",
        "\n",
        "  y_train = pd.Series([score_to_class[s] for s in y_train]).reset_index(drop=True)\n",
        "  y_test = pd.Series([score_to_class[s] for s in y_test]).reset_index(drop=True)\n",
        "\n",
        "  min_score = min(min(y_train), min(y_test))\n",
        "  max_score = max(max(y_train), max(y_test))\n",
        "\n",
        "\n",
        "\n",
        "  def make_compute_metrics(min_score, max_score):\n",
        "    def compute_metrics(eval_pred):\n",
        "        logits, labels = eval_pred\n",
        "        preds = np.argmax(logits, axis=-1)\n",
        "        true_scores = [class_to_score[i] for i in labels]\n",
        "        pred_scores = [class_to_score[i] for i in preds]\n",
        "        qwk = cohen_kappa_score(true_scores, pred_scores, weights='quadratic')\n",
        "        acc = accuracy_score(true_scores, pred_scores)\n",
        "        return {'eval_qwk': qwk, 'accuracy': acc}\n",
        "    return compute_metrics\n",
        "\n",
        "  def tokenize_texts(text):\n",
        "    cleaned_texts = []\n",
        "    for t in text:\n",
        "      t = t.replace('\\n', ' ').replace('\\r', ' ').replace('\\t', ' ')\n",
        "      t = re.sub(r'\\s+', ' ', t)\n",
        "      t = t.strip()\n",
        "      cleaned_texts.append(t)\n",
        "\n",
        "    return tokenizer(cleaned_texts, padding='max_length', truncation=True, max_length=512, return_tensors='pt')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  binned_y = pd.qcut(y_train, q=5, duplicates='drop', labels = False)\n",
        "  kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "  preds = np.zeros((len(y_train), num_classes))\n",
        "  test_preds = []\n",
        "\n",
        "\n",
        "  X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(X_train, y_train, test_size=0.2, random_state=42, stratify = y_train)\n",
        "\n",
        "  X_train_split = X_train_split.tolist()\n",
        "  X_val_split = X_val_split.tolist()\n",
        "  y_train_split = y_train_split.tolist()\n",
        "  y_val_split = y_val_split.tolist()\n",
        "  X_test = X_test.tolist()\n",
        "  y_test = y_test.tolist()\n",
        "\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "  X_train_split_encoding = tokenize_texts(X_train_split)\n",
        "  X_test_encoding = tokenize_texts(X_test)\n",
        "  X_val_split_encoding = tokenize_texts(X_val_split)\n",
        "\n",
        "\n",
        "  X_train_split_dataset = EssayDatasetClassification(X_train_split_encoding, y_train_split)\n",
        "  X_test_dataset = EssayDatasetClassification(X_test_encoding, y_test)\n",
        "  X_val_split_dataset = EssayDatasetClassification(X_val_split_encoding, y_val_split)\n",
        "  # true_labels = y_test.tolist()\n",
        "\n",
        "  model2 = AutoModelForSequenceClassification.from_pretrained(model_name, problem_type='single_label_classification', num_labels = num_classes)\n",
        "\n",
        "  for name, param in model2.named_parameters():\n",
        "      param.requires_grad = False\n",
        "\n",
        "  for name, param in model2.named_parameters():\n",
        "      if any(layer in name for layer in ['encoder.layer.8', 'encoder.layer.9','encoder.layer.10', 'encoder.layer.11', 'pooler', 'classifier']):\n",
        "          param.requires_grad = True\n",
        "\n",
        "\n",
        "\n",
        "  training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    learning_rate=lr,\n",
        "    per_device_train_batch_size=bs,\n",
        "    per_device_eval_batch_size=bs,\n",
        "    num_train_epochs= epochs,\n",
        "    logging_strategy = 'epoch',\n",
        "    eval_strategy = 'epoch',\n",
        "    save_strategy = 'epoch',\n",
        "    load_best_model_at_end = True,\n",
        "    metric_for_best_model = 'eval_qwk',\n",
        "    greater_is_better=True,\n",
        "    report_to = 'none',\n",
        "\n",
        "\n",
        "  )\n",
        "\n",
        "\n",
        "  trainer2 = Trainer(\n",
        "      model=model2,\n",
        "      args=training_args,\n",
        "      train_dataset=X_train_split_dataset,\n",
        "      compute_metrics = make_compute_metrics(min_score,max_score),\n",
        "      eval_dataset=X_val_split_dataset,\n",
        "      callbacks = [EarlyStoppingCallback(early_stopping_patience=3)]\n",
        "  )\n",
        "\n",
        "  trainer2.train()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  results_test = trainer2.predict(X_test_dataset)\n",
        "  test_preds_output = np.argmax(results_test.predictions, axis=1)\n",
        "  test_pred_scores = [class_to_score[i] for i in test_preds_output.astype(int)]\n",
        "  true_test_scores = [class_to_score[i] for i in results_test.label_ids]\n",
        "  qwk_whole_test = cohen_kappa_score(test_pred_scores, true_test_scores, weights='quadratic')\n",
        "  print(f'QWK: {qwk_whole_test}')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#   ### This code below is for hyper-parameter tuning\n",
        "\n",
        "\n",
        "def generate_stacking_preds_hp_tune(model_name, X_train, X_test, y_train, y_test, lr = 0.00001, bs = 16, epochs = 30):\n",
        "  class EssayDatasetClassification(Dataset):\n",
        "        def __init__(self, encodings, labels, seq_len = 512):\n",
        "            self.encodings = encodings\n",
        "            self.labels = labels\n",
        "            self.seq_len = 512\n",
        "\n",
        "        def __getitem__(self, item):\n",
        "            return {\n",
        "                'input_ids': self.encodings['input_ids'][item].clone().detach(),\n",
        "                'attention_mask': self.encodings['attention_mask'][item].clone().detach(),\n",
        "                'labels': torch.tensor(self.labels[item], dtype=torch.long)\n",
        "            }\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self.labels)\n",
        "\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "\n",
        "  X_train = X_train.reset_index(drop=True)\n",
        "  y_train = y_train.reset_index(drop=True)\n",
        "\n",
        "  X_test = X_test.reset_index(drop=True)\n",
        "  y_test = y_test.reset_index(drop=True)\n",
        "\n",
        "  unique_scores = sorted(list(set(np.concatenate([np.array(y_train), np.array(y_test)]))))\n",
        "  score_to_class = {score: i for i, score in enumerate(unique_scores)}\n",
        "  class_to_score = {i: score for score, i in score_to_class.items()}\n",
        "  num_classes = len(score_to_class)\n",
        "\n",
        "  y_train = [score_to_class[int(s)] for s in y_train]\n",
        "  y_test = [score_to_class[int(s)] for s in y_test]\n",
        "\n",
        "  min_score = min(min(y_train), min(y_test))\n",
        "  max_score = max(max(y_train), max(y_test))\n",
        "\n",
        "  def make_compute_metrics(min_score, max_score):\n",
        "    def compute_metrics(eval_pred):\n",
        "        logits, labels = eval_pred\n",
        "        preds = np.argmax(logits, axis=-1)\n",
        "        true_scores = [class_to_score[i] for i in labels]\n",
        "        pred_scores = [class_to_score[i] for i in preds]\n",
        "        qwk = cohen_kappa_score(true_scores, pred_scores, weights='quadratic')\n",
        "        acc = accuracy_score(true_scores, pred_scores)\n",
        "        return {'eval_qwk': qwk, 'accuracy': acc}\n",
        "    return compute_metrics\n",
        "\n",
        "  def tokenize_texts(text):\n",
        "        return tokenizer(text, padding='max_length', truncation=True, max_length=512, return_tensors='pt')\n",
        "\n",
        "\n",
        "\n",
        "  model = AutoModelForSequenceClassification.from_pretrained(model_name, problem_type = 'single_label_classification', num_labels = num_classes)\n",
        "\n",
        "  X_train_ft, X_val, y_train_ft, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42, stratify = y_train)\n",
        "\n",
        "\n",
        "  train_data = X_train_ft.tolist()\n",
        "  val_data = X_val.tolist()\n",
        "  train_labels = y_train_ft\n",
        "  val_labels = y_val\n",
        "\n",
        "  train_encoding = tokenize_texts(train_data)\n",
        "  val_encoding = tokenize_texts(val_data)\n",
        "\n",
        "\n",
        "  train_dataset = EssayDatasetClassification(train_encoding, train_labels)\n",
        "  val_dataset = EssayDatasetClassification(val_encoding, val_labels)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  for name, param in model.named_parameters():\n",
        "      param.requires_grad = False\n",
        "\n",
        "  for name, param in model.named_parameters():\n",
        "    if any(layer in name for layer in ['encoder.layer.8', 'encoder.layer.9','encoder.layer.10', 'encoder.layer.11', 'pooler', 'classifier']):\n",
        "        param.requires_grad = True\n",
        "\n",
        "        # min_score = min(y_train)\n",
        "        # max_score = max(y_train)\n",
        "\n",
        "\n",
        "  training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    learning_rate=lr,\n",
        "    per_device_train_batch_size=bs,\n",
        "    per_device_eval_batch_size=bs,\n",
        "    num_train_epochs= epochs,\n",
        "    logging_strategy = 'epoch',\n",
        "    eval_strategy = 'epoch',\n",
        "    save_strategy = 'epoch',\n",
        "    load_best_model_at_end = True,\n",
        "    metric_for_best_model = 'eval_qwk',\n",
        "    greater_is_better=True,\n",
        "    report_to = 'none',\n",
        "\n",
        "\n",
        "  )\n",
        "\n",
        "\n",
        "\n",
        "  trainer = Trainer(\n",
        "      model=model,\n",
        "      args=training_args,\n",
        "      train_dataset=train_dataset,\n",
        "      eval_dataset=val_dataset,\n",
        "      compute_metrics=make_compute_metrics(min_score, max_score),\n",
        "      callbacks = [EarlyStoppingCallback(early_stopping_patience=3)]\n",
        "  )\n",
        "\n",
        "  trainer.train()\n",
        "\n",
        "  results = trainer.predict(val_dataset)\n",
        "  val_preds = np.argmax(results.predictions, axis=1)\n",
        "  val_pred_scores = [class_to_score[i] for i in val_preds.astype(int)]\n",
        "  true_val_scores = [class_to_score[i] for i in results.label_ids]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  qwk_val = cohen_kappa_score(val_pred_scores, true_val_scores, weights='quadratic')\n",
        "  print(f'QWK: {qwk_val}')\n",
        "\n",
        "  return qwk_val\n",
        "\n",
        "def tune_hyperparameters(trial):\n",
        "    lr = trial.suggest_float('lr', 1e-5, 5e-5, step=1e-5)\n",
        "    bs = trial.suggest_categorical('batch_size', [8, 16, 32])\n",
        "\n",
        "    print(f'Trial {trial.number}: lr={lr}, batch_size={bs}')\n",
        "\n",
        "\n",
        "\n",
        "    qwk  = generate_stacking_preds_hp_tune('bert-base-uncased', X_train_all, X_test_all, y_train_all, y_test_all, lr=lr, bs = bs)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    return qwk\n",
        "\n",
        "\n",
        "# study = optuna.create_study(direction='maximize')\n",
        "# study.optimize(tune_hyperparameters, n_trials=10)\n",
        "# print('Number of finished trials:', len(study.trials))\n",
        "# print('Best trial:', study.best_trial.params)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMpnZR6pO7vy"
      },
      "source": [
        "### Checking the run time and performance for the BERT-based model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ox_UKkV2Uqj3"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "start_time = time.time()\n",
        "generate_stacking_preds_classification('bert-base-uncased', X_train_all, X_test_all, y_train_all, y_test_all, lr=0.00005, bs=8)\n",
        "end_time = time.time()\n",
        "print(f'Time taken: {end_time - start_time} seconds')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYrQwNs3P6oL"
      },
      "source": [
        "### Creating the LSTM-based model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xieU7zUCdAX5"
      },
      "outputs": [],
      "source": [
        "from tkinter.constants import W\n",
        "import gensim.downloader as api\n",
        "from gensim.models import KeyedVectors\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from collections import Counter\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "import optuna\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "from nltk import word_tokenize\n",
        "import re\n",
        "# Prompt 1 optim params: {'hidden_dim': 256, 'lr': 0.00040147129315448076, 'weight_decay': 4.194966656572804e-05, 'dropout1': 0.48972751622541333, 'dropout2': 0.5637289114635329, 'dropout3': 0.4006279296851355, 'batch_size': 64}\n",
        "# Prompt 2 Best Params: {'hidden_dim': 128, 'lr': 0.0008329626514872093, 'weight_decay': 1.10904162552094e-06, 'dropout1': 0.4113129372112616, 'dropout2': 0.5339519738927687, 'dropout3': 0.6474745827693765, 'batch_size': 32}\n",
        "# Prompt 3 Best Params: {'hidden_dim': 64, 'lr': 0.0009176211868710996, 'weight_decay': 8.47468900298905e-05, 'dropout1': 0.4402196829382994, 'dropout2': 0.6543812752462479, 'dropout3': 0.30572393075878385, 'batch_size': 128}\n",
        "# Prompt 4 Best Params: Best trial: {'hidden_dim': 256, 'lr': 0.0006392739979334983, 'weight_decay': 1.0246979767749631e-06, 'dropout1': 0.6331948007619779, 'dropout2': 0.3024115352348601, 'dropout3': 0.4826430525213473, 'batch_size': 32}\n",
        "# Prompt 5 Best Params: {'hidden_dim': 256, 'lr': 0.0007888666643834901, 'weight_decay': 1.9973208515938436e-05, 'dropout1': 0.5150492631843749, 'dropout2': 0.6836007071470228, 'dropout3': 0.44931934750457436, 'batch_size': 64}\n",
        "# Prompt 6 Best Params: {'hidden_dim': 256, 'lr': 0.0004221715369392723, 'weight_decay': 1.6693933987910344e-06, 'dropout1': 0.642989336447883, 'dropout2': 0.6149595736926003, 'dropout3': 0.4561023529421714, 'batch_size': 16}\n",
        "# Prompt 7 Best Params: {'hidden_dim': 128, 'lr': 0.0007031295542079031, 'weight_decay': 1.1167323499868158e-05, 'dropout1': 0.43014203679075136, 'dropout2': 0.6072345773651249, 'dropout3': 0.5301384805467484, 'batch_size': 64}\n",
        "# Prompt 8 Best Params: {'hidden_dim': 256, 'lr': 0.00044054250769868446, 'weight_decay': 1.8209940215624457e-05, 'dropout1': 0.6820871115494689, 'dropout2': 0.5530971947983455, 'dropout3': 0.6417406365573428, 'batch_size': 64}\n",
        "def lstm_model_classification(X_train, X_test, y_train, y_test):\n",
        "    w2v_model = api.load('word2vec-google-news-300')\n",
        "    embedding_size = w2v_model.vector_size\n",
        "\n",
        "    X_train = X_train.reset_index(drop=True)\n",
        "    y_train = y_train.reset_index(drop=True)\n",
        "\n",
        "    X_test = X_test.reset_index(drop=True)\n",
        "    y_test = y_test.reset_index(drop=True)\n",
        "\n",
        "    unique_scores = sorted(list(set(np.concatenate([np.array(y_train), np.array(y_test)]))))\n",
        "    score_to_class = {score: i for i, score in enumerate(unique_scores)}\n",
        "    class_to_score = {i: score for score, i in score_to_class.items()}\n",
        "    num_classes = len(score_to_class)\n",
        "\n",
        "    y_train = pd.Series([score_to_class[s] for s in y_train])\n",
        "    y_test = pd.Series([score_to_class[s] for s in y_test])\n",
        "\n",
        "    y_train = y_train.reset_index(drop=True)\n",
        "    y_test = y_test.reset_index(drop=True)\n",
        "\n",
        "    min_score = min(min(y_train), min(y_test))\n",
        "    max_score = max(max(y_train), max(y_test))\n",
        "\n",
        "\n",
        "    class Vocab:\n",
        "        def __init__(self, token_freqs, min_freq=1, specials=['<pad>', '<unk>']):\n",
        "            self.itos = list(specials)\n",
        "            self.stoi = {tok: i for i, tok in enumerate(self.itos)}\n",
        "            for token, freq in token_freqs.items():\n",
        "                if freq >= min_freq and token not in self.stoi:\n",
        "                    self.stoi[token] = len(self.itos)\n",
        "                    self.itos.append(token)\n",
        "        def __len__(self):\n",
        "            return len(self.itos)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    class EssayDataset(Dataset):\n",
        "        def __init__(self, essays, labels, max_len, vocab):\n",
        "            self.essays = essays\n",
        "            self.labels = labels\n",
        "            self.max_length = max_len\n",
        "            self.vocab = vocab\n",
        "\n",
        "        def create_encodings(self, text):\n",
        "            word_token = word_tokenize(text.lower())\n",
        "            input_ids = [self.vocab.stoi.get(word, self.vocab.stoi['<unk>']) for word in word_token]\n",
        "            if len(input_ids) < self.max_length:\n",
        "                input_ids += [self.vocab.stoi['<pad>']] * (self.max_length - len(input_ids))\n",
        "            else:\n",
        "                input_ids = input_ids[:self.max_length]\n",
        "            return torch.tensor(input_ids)\n",
        "\n",
        "        def __getitem__(self, index):\n",
        "            essay = self.create_encodings(self.essays[index])\n",
        "            label = self.labels[index]\n",
        "            return essay, label\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self.essays)\n",
        "\n",
        "    class BiLSTM_CNN(nn.Module):\n",
        "        def __init__(self, vocab_size, embedding_dim, hidden_dim, embedding_matrix):\n",
        "            super().__init__()\n",
        "            self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
        "            self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
        "            self.conv1 = nn.Conv1d(hidden_dim * 2, hidden_dim, kernel_size=3, padding=1)\n",
        "            self.dropout1 = nn.Dropout(0.4402196829382994)\n",
        "            self.conv2 = nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, padding=1)\n",
        "            self.dropout2 = nn.Dropout(0.6543812752462479)\n",
        "            self.pool = nn.AdaptiveAvgPool1d(1)\n",
        "            self.dropout3 = nn.Dropout(0.30572393075878385)\n",
        "            self.fc = nn.Linear(hidden_dim, num_classes)\n",
        "\n",
        "        def forward(self, x):\n",
        "            embedded = self.embedding(x)\n",
        "            lstm_out, _ = self.lstm(embedded)\n",
        "            conv1_out = torch.relu(self.conv1(lstm_out.permute(0, 2, 1)))\n",
        "            conv2_out = torch.relu(self.conv2(conv1_out))\n",
        "            pool_out = self.pool(conv2_out).squeeze(2)\n",
        "            output = self.fc(pool_out)\n",
        "            return output\n",
        "\n",
        "    def cleaning_data(text):\n",
        "      text = text.lower()\n",
        "      text = text.replace('\\n', ' ')\n",
        "      text = text.replace('\\t', ' ')\n",
        "      text = text.replace('\\r', ' ')\n",
        "      text = re.sub(r'\\s+', ' ', text)\n",
        "      text = text.strip()\n",
        "      return text\n",
        "\n",
        "    essay_data = X_train.tolist() + X_test.tolist()\n",
        "    essays = [cleaning_data(essay) for essay in essay_data]\n",
        "    X_train = pd.Series(essays[:len(X_train)])\n",
        "    X_test = pd.Series(essays[len(X_train):])\n",
        "    counter = Counter()\n",
        "    for essay in essays:\n",
        "        counter.update(word_tokenize(essay.lower()))\n",
        "    most_common = counter.most_common(4000)\n",
        "    vocab = Vocab(dict(most_common), min_freq=1)\n",
        "\n",
        "    embedding_matrix = np.zeros((len(vocab), embedding_size))\n",
        "    for i, word in enumerate(vocab.itos):\n",
        "        embedding_matrix[i] = w2v_model[word] if word in w2v_model else np.random.normal(scale=0.6, size=(embedding_size,))\n",
        "    embedding_matrix = torch.tensor(embedding_matrix, dtype=torch.float32)\n",
        "\n",
        "    min_score = int(min(y_train.min(), y_test.min()))\n",
        "    max_score = int(max(y_train.max(), y_test.max()))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # scaler = MinMaxScaler()\n",
        "    # scaler.fit(y_train.values.reshape(-1, 1))\n",
        "\n",
        "    # Comment this out to do fine-tuning\n",
        "    # def tuning_parameters(trial):\n",
        "    #   split = int(0.8 * len(X_train))\n",
        "    #   X_tr, X_val = X_train[:split].reset_index(drop=True), X_train[split:].reset_index(drop=True)\n",
        "    #   y_tr, y_val = y_train[:split].reset_index(drop=True), y_train[split:].reset_index(drop=True)\n",
        "\n",
        "    #   hidden_dim = trial.suggest_categorical('hidden_dim', [64, 128, 256, 512])\n",
        "    #   lr = trial.suggest_float('lr', 1e-5, 1e-3, log = True)\n",
        "    #   weight_decay = trial.suggest_float('weight_decay', 1e-6, 1e-4, log=True)\n",
        "    #   dropout1 = trial.suggest_float('dropout1', 0.2, 0.8)\n",
        "    #   dropout2 = trial.suggest_float('dropout2', 0.2, 0.8)\n",
        "    #   dropout3 = trial.suggest_float('dropout3', 0.2, 0.8)\n",
        "    #   batch_size = trial.suggest_categorical('batch_size', [16, 32, 64, 128])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    #   full_train_dataset = EssayDataset(X_tr.tolist(), y_tr, 512, vocab)\n",
        "    #   full_train_loader = DataLoader(full_train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    #   full_val_dataset = EssayDataset(X_val.tolist(), y_val, 512, vocab)\n",
        "    #   full_val_loader = DataLoader(full_val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    #   model = BiLSTM_CNN(len(vocab), embedding_size, hidden_dim, embedding_matrix)\n",
        "    #   model.dropout1.p = dropout1\n",
        "    #   model.dropout2.p = dropout2\n",
        "    #   model.dropout3.p = dropout3\n",
        "    #   optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    #   criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    #   for epoch in range(5):\n",
        "    #       model.train()\n",
        "    #       total_loss = 0\n",
        "    #       for essays, labels in full_train_loader:\n",
        "    #           optimizer.zero_grad()\n",
        "    #           output = model(essays)\n",
        "    #           loss = criterion(output, labels.long())\n",
        "    #           total_loss += loss.item()\n",
        "    #           loss.backward()\n",
        "    #           optimizer.step()\n",
        "\n",
        "    #   model.eval()\n",
        "    #   val_preds = []\n",
        "    #   with torch.no_grad():\n",
        "    #       for essays, _ in full_val_loader:\n",
        "    #           output = model(essays).cpu().numpy()\n",
        "    #           val_preds.extend(torch.argmax(torch.tensor(output), dim=1).numpy())\n",
        "    #   val_pred_scores = [class_to_score[i] for i in val_preds]\n",
        "    #   val_true_scores = [class_to_score[i] for i in y_val.tolist()]\n",
        "    #   qwk = cohen_kappa_score(val_pred_scores, val_true_scores, weights='quadratic')\n",
        "    #   print(f'QWK: {qwk}')\n",
        "    #   return qwk\n",
        "\n",
        "    # study = optuna.create_study(direction='maximize')\n",
        "    # study.optimize(tuning_parameters, n_trials=10)\n",
        "    # print('Number of finished trials:', len(study.trials))\n",
        "    # print('Best trial:', study.best_trial.params)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    full_train_dataset = EssayDataset(X_train.tolist(), y_train, 512, vocab)\n",
        "    full_train_loader = DataLoader(full_train_dataset, batch_size=128, shuffle=True)\n",
        "\n",
        "    model2 = BiLSTM_CNN(len(vocab), embedding_size, 64, embedding_matrix)\n",
        "    optimizer = torch.optim.Adam(model2.parameters(), lr = 0.0009176211868710996, weight_decay = 8.47468900298905e-05)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(10):\n",
        "        model2.train()\n",
        "        total_loss = 0\n",
        "        for essays, labels in full_train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            output = model2(essays)\n",
        "            loss = criterion(output, labels.long())\n",
        "            total_loss += loss.item()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        print(f'Epoch {epoch + 1}, Loss: {total_loss / len(full_train_loader)}')\n",
        "\n",
        "\n",
        "    model2.eval()\n",
        "    test_dataset = EssayDataset(X_test.tolist(), np.zeros(len(X_test)), 512, vocab)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
        "\n",
        "    final_test_preds = []\n",
        "    with torch.no_grad():\n",
        "        for essays, _ in test_loader:\n",
        "            output = model2(essays).cpu().numpy()\n",
        "            output = torch.argmax(torch.tensor(output), dim=1).numpy()\n",
        "            final_test_preds.extend(output)\n",
        "\n",
        "    final_test_preds = [class_to_score[i] for i in final_test_preds]\n",
        "    true_scores = [class_to_score[i] for i in y_test.tolist()]\n",
        "\n",
        "\n",
        "    qwk = cohen_kappa_score(final_test_preds, true_scores, weights = 'quadratic')\n",
        "    print(f'QWK : {qwk}')\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PKPmD_PfnQG"
      },
      "source": [
        "### Checking the run time and performance for the LSTM-based model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XJHFWRgVuEyy"
      },
      "outputs": [],
      "source": [
        "start_time = time.time()\n",
        "lstm_model_classification(X_train_all, X_test_all, y_train_all, y_test_all)\n",
        "end_time = time.time()\n",
        "print(f'Time taken: {end_time - start_time} seconds')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fg3NtkaSRWwN"
      },
      "source": [
        "### Creating the Random Forest model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QCv_8HLfIkKF"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from sklearn.base import clone\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import spacy\n",
        "from spellchecker import SpellChecker\n",
        "\n",
        "def random_forest_model_with_gridsearch(X_train, X_test, y_train, y_test):\n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "    spell = SpellChecker()\n",
        "\n",
        "    def cleaning_data(text):\n",
        "      text = text.replace('\\n', ' ')\n",
        "      text = text.replace('\\t', ' ')\n",
        "      text = text.replace('\\r', ' ')\n",
        "      text = re.sub(r'\\s+', ' ', text)\n",
        "      text = text.strip()\n",
        "      return text\n",
        "\n",
        "    X_train = X_train.apply(cleaning_data)\n",
        "    X_test = X_test.apply(cleaning_data)\n",
        "\n",
        "    def feature_creation(essay):\n",
        "        doc = nlp(essay)\n",
        "        num_sentences = len(list(doc.sents))\n",
        "        num_words = len(essay.split())\n",
        "        num_unique_words = len(set(essay.split()))\n",
        "        num_stop_words = len([token for token in doc if token.is_stop])\n",
        "        num_proper_nouns = len([token for token in doc if token.pos_ == 'PROPN'])\n",
        "        num_verbs = len([token for token in doc if token.pos_ == 'VERB'])\n",
        "        num_adjectives = len([token for token in doc if token.pos_ == 'ADJ'])\n",
        "        num_adverbs = len([token for token in doc if token.pos_ == 'ADV'])\n",
        "        num_nouns = len([token for token in doc if token.pos_ == 'NOUN'])\n",
        "        num_prepositions = len([token for token in doc if token.pos_ == 'ADP'])\n",
        "        num_pronouns = len([token for token in doc if token.pos_ == 'PRON'])\n",
        "        num_conjunctions = len([token for token in doc if token.pos_ == 'CCONJ'])\n",
        "        num_interjections = len([token for token in doc if token.pos_ == 'INTJ'])\n",
        "        num_punctuation = len([token for token in doc if token.pos_ == 'PUNCT'])\n",
        "        num_digits = len([token for token in doc if token.pos_ == 'NUM'])\n",
        "        num_entities = len(list(doc.ents))\n",
        "        num_spelling_errors = len(spell.unknown(essay.split()))\n",
        "        avg_word_length = np.mean([len(word) for word in essay.split()])\n",
        "        avg_sentence_length = np.mean([len(sent) for sent in list(doc.sents)])\n",
        "        return [\n",
        "            num_sentences, num_words, num_unique_words, num_stop_words, num_proper_nouns,\n",
        "            num_verbs, num_adjectives, num_adverbs, num_nouns, num_prepositions,\n",
        "            num_pronouns, num_conjunctions, num_interjections, num_punctuation, num_digits,\n",
        "            num_entities, num_spelling_errors, avg_word_length, avg_sentence_length\n",
        "        ]\n",
        "\n",
        "    features = np.array([feature_creation(essay.strip()) for essay in X_train])\n",
        "    features_test = np.array([feature_creation(essay.strip()) for essay in X_test])\n",
        "\n",
        "    unique_scores = sorted(list(set(np.concatenate([np.array(y_train), np.array(y_test)]))))\n",
        "    score_to_class = {score: i for i, score in enumerate(unique_scores)}\n",
        "    class_to_score = {i: score for score, i in score_to_class.items()}\n",
        "\n",
        "    y_train_class = y_train.map(score_to_class)\n",
        "    y_test_class = y_test.map(score_to_class)\n",
        "\n",
        "    binned_y = pd.qcut(y_train_class, q=5, duplicates='drop', labels=False)\n",
        "    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "    oof_preds = np.zeros((len(y_train), len(score_to_class)))\n",
        "    y_train_values = np.zeros(len(y_train))\n",
        "    test_probs_folds = []\n",
        "\n",
        "    param_grid = {\n",
        "        'n_estimators': [100, 300, 500],\n",
        "        'max_depth': [None, 10, 20, 50],\n",
        "        'min_samples_split': [2, 5, 7],\n",
        "        'max_features': ['sqrt', 'log2']\n",
        "    }\n",
        "\n",
        "    grid_search = GridSearchCV(\n",
        "        RandomForestClassifier(random_state=42),\n",
        "        param_grid,\n",
        "        cv=3,\n",
        "        n_jobs=-1,\n",
        "        scoring='accuracy'\n",
        "    )\n",
        "    grid_search.fit(features, y_train_class)\n",
        "    print('Best Parameters:', grid_search.best_params_)\n",
        "\n",
        "    best_model = grid_search.best_estimator_\n",
        "\n",
        "\n",
        "    model2 = clone(best_model)\n",
        "    model2.fit(features, y_train_class)\n",
        "\n",
        "    final_test_preds = model2.predict(features_test)\n",
        "    final_test_scores_preds = [class_to_score[i] for i in final_test_preds]\n",
        "    true_test_scores = [class_to_score[i] for i in y_test_class]\n",
        "    qwk = cohen_kappa_score(final_test_scores_preds, true_test_scores, weights='quadratic')\n",
        "    print(f'QWK: {qwk}')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_VuM6GqRlmN"
      },
      "source": [
        "### Checking the run time and performance for the Random Forest model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VgtCX7ZGI445"
      },
      "outputs": [],
      "source": [
        "start_time = time.time()\n",
        "random_forest_model_with_gridsearch(X_train_all, X_test_all, y_train_all, y_test_all)\n",
        "end_time = time.time()\n",
        "print(f'Time taken: {end_time - start_time} seconds')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXiuyBtVRz_h"
      },
      "source": [
        "# Creating the stacking set up for regression models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJNGfa0uR47i"
      },
      "source": [
        "### Creating the BERT-based model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nSSR1OfWFYaF"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "import optuna\n",
        "\n",
        "\n",
        "# Prompt 1 Best Params:  {'lr': 5e-05, 'batch_size': 8}\n",
        "# Prompt 7 Best Params: {'lr': 3e-05, 'batch_size': 8}\n",
        "# Prompt 8 Best Params: {'lr': 5e-05, 'batch_size': 8}\n",
        "def generate_stacking_preds_time(model_name, X_train, X_test, y_train, y_test, lr = 0.00001, bs = 16, epochs = 30):\n",
        "  class EssayDatasetRegression(Dataset):\n",
        "            def __init__(self, encodings, labels):\n",
        "                self.encodings = encodings\n",
        "                self.labels = labels\n",
        "\n",
        "            def __getitem__(self, idx):\n",
        "                item = {key: val[idx].clone().detach() for key, val in self.encodings.items()}\n",
        "                item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float)\n",
        "                return item\n",
        "\n",
        "            def __len__(self):\n",
        "                return len(self.labels)\n",
        "\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "\n",
        "  X_train = X_train.reset_index(drop=True)\n",
        "  y_train = y_train.reset_index(drop=True)\n",
        "\n",
        "  X_test = X_test.reset_index(drop=True)\n",
        "  y_test = y_test.reset_index(drop=True)\n",
        "\n",
        "  min_score = min(min(y_train), min(y_test))\n",
        "  max_score = max(max(y_train), max(y_test))\n",
        "\n",
        "  def make_compute_metrics(min_score, max_score):\n",
        "    def compute_metrics(eval_pred):\n",
        "          preds, labels = eval_pred\n",
        "          preds_rounded = np.clip(np.rint(preds), min_score, max_score).astype(int)\n",
        "          labels_rounded = np.rint(labels).astype(int)\n",
        "          return {'eval_qwk': cohen_kappa_score(preds_rounded, labels_rounded, weights='quadratic')}\n",
        "    return compute_metrics\n",
        "\n",
        "  def tokenize_texts(text):\n",
        "        cleaned_texts = []\n",
        "        for t in text:\n",
        "          t = t.replace('\\n', ' ').replace('\\r', ' ').replace('\\t', ' ')\n",
        "          t = re.sub(r'\\s+', ' ', t)\n",
        "          t = t.strip()\n",
        "          cleaned_texts.append(t)\n",
        "        return tokenizer(cleaned_texts, padding='max_length', truncation=True, max_length=512, return_tensors='pt')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  binned_y = pd.qcut(y_train, q=5, duplicates='drop', labels = False)\n",
        "  kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "  preds = np.zeros(len(X_train))\n",
        "  test_preds = []\n",
        "\n",
        "  for train_idx, val_idx in kf.split(X_train, binned_y):\n",
        "          model = AutoModelForSequenceClassification.from_pretrained(model_name, problem_type = 'regression', num_labels=1)\n",
        "          train_data = X_train.iloc[train_idx].tolist()\n",
        "          val_data = X_train.iloc[val_idx].tolist()\n",
        "          train_labels = y_train.iloc[train_idx].tolist()\n",
        "          val_labels = y_train.iloc[val_idx].tolist()\n",
        "\n",
        "          train_encoding = tokenize_texts(train_data)\n",
        "          val_encoding = tokenize_texts(val_data)\n",
        "\n",
        "          test_encoding = tokenize_texts(X_test.tolist())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "          train_dataset = EssayDatasetRegression(train_encoding, train_labels)\n",
        "          test_dataset = EssayDatasetRegression(test_encoding, y_test)\n",
        "          val_dataset = EssayDatasetRegression(val_encoding, val_labels)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "          for name, param in model.named_parameters():\n",
        "              param.requires_grad = False\n",
        "\n",
        "          for name, param in model.named_parameters():\n",
        "            if any(layer in name for layer in ['encoder.layer.8', 'encoder.layer.9','encoder.layer.10', 'encoder.layer.11', 'pooler', 'classifier']):\n",
        "                param.requires_grad = True\n",
        "\n",
        "          # min_score = min(y_train)\n",
        "          # max_score = max(y_train)\n",
        "\n",
        "\n",
        "          training_args = TrainingArguments(\n",
        "            output_dir='./results',\n",
        "            learning_rate=lr,\n",
        "            per_device_train_batch_size=bs,\n",
        "            per_device_eval_batch_size=bs,\n",
        "            num_train_epochs= epochs,\n",
        "            logging_strategy = 'epoch',\n",
        "            eval_strategy = 'epoch',\n",
        "            save_strategy = 'epoch',\n",
        "            load_best_model_at_end = True,\n",
        "            metric_for_best_model = 'eval_loss',\n",
        "            greater_is_better=False,\n",
        "            report_to = 'none',\n",
        "\n",
        "\n",
        "        )\n",
        "\n",
        "      #   train_dataset = EssayDatasetRegression(train_encoding, y_train)\n",
        "      #   test_dataset = EssayDatasetRegression(test_encoding, y_test)\n",
        "      #   val_dataset = EssayDatasetRegression(val_encoding, y_val)\n",
        "\n",
        "          trainer = Trainer(\n",
        "              model=model,\n",
        "              args=training_args,\n",
        "              train_dataset=train_dataset,\n",
        "              eval_dataset=val_dataset,\n",
        "              compute_metrics=make_compute_metrics(min_score, max_score),\n",
        "              callbacks = [EarlyStoppingCallback(early_stopping_patience=5)]\n",
        "          )\n",
        "\n",
        "          trainer.train()\n",
        "\n",
        "          results = trainer.predict(val_dataset)\n",
        "          preds[val_idx] = np.rint(results.predictions.squeeze()).astype(int)\n",
        "\n",
        "          results_test = trainer.predict(test_dataset)\n",
        "          test_preds.append(np.rint(results_test.predictions.squeeze()).astype(int))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  avg_test_preds = np.mean(test_preds, axis=0)\n",
        "  final_test_preds_rounded = np.clip(np.rint(avg_test_preds), min_score, max_score).astype(int)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # return preds_test, y_train, y_test\n",
        "  return preds, final_test_preds_rounded, avg_test_preds, y_train, y_test\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojPxXsCXTjEf"
      },
      "source": [
        "### Creating the LSTM-based model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OLpLwVLcFh0C"
      },
      "outputs": [],
      "source": [
        "import gensim.downloader as api\n",
        "from gensim.models import KeyedVectors\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from collections import Counter\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "import optuna\n",
        "\n",
        "# Prompt 1 optim params: {'hidden_dim': 256, 'lr': 0.00040147129315448076, 'weight_decay': 4.194966656572804e-05, 'dropout1': 0.48972751622541333, 'dropout2': 0.5637289114635329, 'dropout3': 0.4006279296851355, 'batch_size': 64}\n",
        "# Prompt 2 Best Params: {'hidden_dim': 128, 'lr': 0.0008329626514872093, 'weight_decay': 1.10904162552094e-06, 'dropout1': 0.4113129372112616, 'dropout2': 0.5339519738927687, 'dropout3': 0.6474745827693765, 'batch_size': 32}\n",
        "# Prompt 3 Best Params: {'hidden_dim': 64, 'lr': 0.0009176211868710996, 'weight_decay': 8.47468900298905e-05, 'dropout1': 0.4402196829382994, 'dropout2': 0.6543812752462479, 'dropout3': 0.30572393075878385, 'batch_size': 128}\n",
        "# Prompt 4 Best Params: Best trial: {'hidden_dim': 512, 'lr': 0.0006392739979334983, 'weight_decay': 1.0246979767749631e-06, 'dropout1': 0.6331948007619779, 'dropout2': 0.3024115352348601, 'dropout3': 0.4826430525213473, 'batch_size': 32}\n",
        "# Prompt 5 Best Params: {'hidden_dim': 256, 'lr': 0.0007888666643834901, 'weight_decay': 1.9973208515938436e-05, 'dropout1': 0.5150492631843749, 'dropout2': 0.6836007071470228, 'dropout3': 0.44931934750457436, 'batch_size': 16}\n",
        "# Prompt 6 Best Params: {'hidden_dim': 256, 'lr': 0.0004221715369392723, 'weight_decay': 1.6693933987910344e-06, 'dropout1': 0.642989336447883, 'dropout2': 0.6149595736926003, 'dropout3': 0.4561023529421714, 'batch_size': 16}\n",
        "# Prompt 7 Best Params: {'hidden_dim': 128, 'lr': 0.0007031295542079031, 'weight_decay': 1.1167323499868158e-05, 'dropout1': 0.43014203679075136, 'dropout2': 0.6072345773651249, 'dropout3': 0.5301384805467484, 'batch_size': 64}\n",
        "# Prompt 8 Best Params: {'hidden_dim': 256, 'lr': 0.00044054250769868446, 'weight_decay': 1.8209940215624457e-05, 'dropout1': 0.6820871115494689, 'dropout2': 0.5530971947983455, 'dropout3': 0.6417406365573428, 'batch_size': 64}\n",
        "def lstm_model_time(X_train, X_test, y_train, y_test):\n",
        "    w2v_model = api.load('word2vec-google-news-300')\n",
        "    embedding_size = w2v_model.vector_size\n",
        "\n",
        "    class Vocab:\n",
        "        def __init__(self, token_freqs, min_freq=1, specials=['<pad>', '<unk>']):\n",
        "            self.itos = list(specials)\n",
        "            self.stoi = {tok: i for i, tok in enumerate(self.itos)}\n",
        "            for token, freq in token_freqs.items():\n",
        "                if freq >= min_freq and token not in self.stoi:\n",
        "                    self.stoi[token] = len(self.itos)\n",
        "                    self.itos.append(token)\n",
        "        def __len__(self):\n",
        "            return len(self.itos)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    class EssayDataset(Dataset):\n",
        "        def __init__(self, essays, labels, max_len, vocab):\n",
        "            self.essays = essays\n",
        "            self.labels = labels\n",
        "            self.max_length = max_len\n",
        "            self.vocab = vocab\n",
        "\n",
        "        def create_encodings(self, text):\n",
        "            word_token = word_tokenize(text.lower())\n",
        "            input_ids = [self.vocab.stoi.get(word, self.vocab.stoi['<unk>']) for word in word_token]\n",
        "            if len(input_ids) < self.max_length:\n",
        "                input_ids += [self.vocab.stoi['<pad>']] * (self.max_length - len(input_ids))\n",
        "            else:\n",
        "                input_ids = input_ids[:self.max_length]\n",
        "            return torch.tensor(input_ids)\n",
        "\n",
        "        def __getitem__(self, index):\n",
        "            essay = self.create_encodings(self.essays[index])\n",
        "            label = self.labels[index]\n",
        "            return essay, label\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self.essays)\n",
        "\n",
        "    class BiLSTM_CNN(nn.Module):\n",
        "        def __init__(self, vocab_size, embedding_dim, hidden_dim, embedding_matrix):\n",
        "            super().__init__()\n",
        "            self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
        "            self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
        "            self.conv1 = nn.Conv1d(hidden_dim * 2, hidden_dim, kernel_size=3, padding=1)\n",
        "            self.dropout1 = nn.Dropout(0.6820871115494689)\n",
        "            self.conv2 = nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, padding=1)\n",
        "            self.dropout2 = nn.Dropout(0.5530971947983455)\n",
        "            self.pool = nn.AdaptiveAvgPool1d(1)\n",
        "            self.dropout3 = nn.Dropout(0.6417406365573428)\n",
        "            self.fc = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "        def forward(self, x):\n",
        "            embedded = self.embedding(x)\n",
        "            lstm_out, _ = self.lstm(embedded)\n",
        "            conv1_out = torch.relu(self.conv1(lstm_out.permute(0, 2, 1)))\n",
        "            conv2_out = torch.relu(self.conv2(conv1_out))\n",
        "            pool_out = self.pool(conv2_out).squeeze(2)\n",
        "            output = self.fc(pool_out)\n",
        "            return output.squeeze()\n",
        "\n",
        "\n",
        "    def cleaning_data(text):\n",
        "      text = text.lower()\n",
        "      text = text.replace('\\n', ' ')\n",
        "      text = text.replace('\\t', ' ')\n",
        "      text = text.replace('\\r', ' ')\n",
        "      text = re.sub(r'\\s+', ' ', text)\n",
        "      text = text.strip()\n",
        "      return text\n",
        "\n",
        "    essay_data = X_train.tolist() + X_test.tolist()\n",
        "    essays = [cleaning_data(essay) for essay in essay_data]\n",
        "    X_train = pd.Series(essays[:len(X_train)])\n",
        "    X_test = pd.Series(essays[len(X_train):])\n",
        "    counter = Counter()\n",
        "    for essay in essays:\n",
        "        counter.update(word_tokenize(essay.lower()))\n",
        "    most_common = counter.most_common(4000)\n",
        "    vocab = Vocab(dict(most_common), min_freq=1)\n",
        "\n",
        "    embedding_matrix = np.zeros((len(vocab), embedding_size))\n",
        "    for i, word in enumerate(vocab.itos):\n",
        "        embedding_matrix[i] = w2v_model[word] if word in w2v_model else np.random.normal(scale=0.6, size=(embedding_size,))\n",
        "    embedding_matrix = torch.tensor(embedding_matrix, dtype=torch.float32)\n",
        "\n",
        "    min_score = int(min(y_train.min(), y_test.min()))\n",
        "    max_score = int(max(y_train.max(), y_test.max()))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    bins = pd.qcut(y_train, q=5, duplicates='drop', labels = False)\n",
        "    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    oof_preds = np.zeros(len(y_train))\n",
        "    y_train_values = np.zeros(len(y_train))\n",
        "    test_preds = []\n",
        "    scaler = MinMaxScaler()\n",
        "    scaler.fit(y_train.values.reshape(-1, 1))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    for train_idx, val_idx in kf.split(X_train, bins):\n",
        "        X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
        "        y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
        "\n",
        "\n",
        "        y_tr_scaled = scaler.transform(y_tr.values.reshape(-1, 1)).flatten()\n",
        "        y_val_scaled = scaler.transform(y_val.values.reshape(-1, 1)).flatten()\n",
        "\n",
        "        train_dataset = EssayDataset(X_tr.tolist(), y_tr_scaled, 512, vocab)\n",
        "        val_dataset = EssayDataset(X_val.tolist(), y_val_scaled, 512, vocab)\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "        model = BiLSTM_CNN(len(vocab), embedding_size, 256, embedding_matrix)\n",
        "        optimizer = torch.optim.Adam(model.parameters(),  lr = 0.00044054250769868446, weight_decay = 1.8209940215624457e-05)\n",
        "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer,\n",
        "        mode='max',\n",
        "        factor=0.5,\n",
        "        patience=1\n",
        "      )\n",
        "        criterion = nn.MSELoss()\n",
        "\n",
        "        for epoch in range(10):\n",
        "            model.train()\n",
        "            total_loss = 0\n",
        "            for essays, labels in train_loader:\n",
        "                optimizer.zero_grad()\n",
        "                output = model(essays)\n",
        "                loss = criterion(output, labels.float())\n",
        "                total_loss += loss.item()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "            print(f'Epoch {epoch + 1}, Loss: {total_loss / len(train_loader)}')\n",
        "\n",
        "        model.eval()\n",
        "        val_preds = []\n",
        "        with torch.no_grad():\n",
        "            for essays, _ in val_loader:\n",
        "                output = model(essays).cpu().numpy()\n",
        "                val_preds.extend(output)\n",
        "\n",
        "        val_preds = scaler.inverse_transform(np.array(val_preds).reshape(-1, 1)).flatten()\n",
        "        val_preds_rounded = np.clip(np.rint(val_preds), min_score, max_score).astype(int)\n",
        "        qwk = cohen_kappa_score(val_preds_rounded, y_val.values, weights='quadratic')\n",
        "\n",
        "\n",
        "        scheduler.step(qwk)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        oof_preds[val_idx] = val_preds_rounded\n",
        "        y_train_values[val_idx] = y_val.values\n",
        "\n",
        "\n",
        "        test_dataset = EssayDataset(X_test.tolist(), y_test.tolist(), 512, vocab)\n",
        "        test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "        model.eval()\n",
        "        fold_test_preds = []\n",
        "        with torch.no_grad():\n",
        "            for essays, _ in test_loader:\n",
        "                output = model(essays).cpu().numpy().flatten()\n",
        "                fold_test_preds.extend(output)\n",
        "\n",
        "        fold_test_preds = scaler.inverse_transform(np.array(fold_test_preds).reshape(-1, 1)).flatten()\n",
        "        test_preds.append(fold_test_preds)\n",
        "\n",
        "\n",
        "    avg_test_preds = np.mean(test_preds, axis=0)\n",
        "    final_preds_rounded = np.clip(np.rint(avg_test_preds), min_score, max_score).astype(int)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    return oof_preds, final_preds_rounded, avg_test_preds, y_train_values, y_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjL29GmSTmlB"
      },
      "source": [
        "### Creating the Random Forest model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EOTBAYhBF54M"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from sklearn.base import clone\n",
        "import spacy\n",
        "from spellchecker import SpellChecker\n",
        "\n",
        "# Prompt 1 Best Params: n_estimators=500, max_depth=10, random_state=42, n_jobs=-1, max_features = 'sqrt', min_samples_split = 2\n",
        "# Prompt 7 Best: Params: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_split': 5, 'n_estimators': 500}\n",
        "# Prompt 8 Best Params:  {'max_depth': 20, 'max_features': 'sqrt', 'min_samples_split': 5, 'n_estimators': 100}\n",
        "\n",
        "\n",
        "def random_forest_model_with_gridsearch_time(X_train, X_test, y_train, y_test):\n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "    spell = SpellChecker()\n",
        "\n",
        "    def cleaning_data(text):\n",
        "      text = text.replace('\\n', ' ')\n",
        "      text = text.replace('\\t', ' ')\n",
        "      text = text.replace('\\r', ' ')\n",
        "      text = re.sub(r'\\s+', ' ', text)\n",
        "      text = text.strip()\n",
        "      return text\n",
        "\n",
        "    X_train = X_train.apply(cleaning_data)\n",
        "    X_test = X_test.apply(cleaning_data)\n",
        "\n",
        "    def feature_creation(essay):\n",
        "        doc = nlp(essay)\n",
        "        num_sentences = len(list(doc.sents))\n",
        "        num_words = len(essay.split())\n",
        "        num_unique_words = len(set(essay.split()))\n",
        "        num_stop_words = len([token for token in doc if token.is_stop])\n",
        "        num_proper_nouns = len([token for token in doc if token.pos_ == 'PROPN'])\n",
        "        num_verbs = len([token for token in doc if token.pos_ == 'VERB'])\n",
        "        num_adjectives = len([token for token in doc if token.pos_ == 'ADJ'])\n",
        "        num_adverbs = len([token for token in doc if token.pos_ == 'ADV'])\n",
        "        num_nouns = len([token for token in doc if token.pos_ == 'NOUN'])\n",
        "        num_prepositions = len([token for token in doc if token.pos_ == 'ADP'])\n",
        "        num_pronouns = len([token for token in doc if token.pos_ == 'PRON'])\n",
        "        num_conjunctions = len([token for token in doc if token.pos_ == 'CCONJ'])\n",
        "        num_interjections = len([token for token in doc if token.pos_ == 'INTJ'])\n",
        "        num_punctuation = len([token for token in doc if token.pos_ == 'PUNCT'])\n",
        "        num_digits = len([token for token in doc if token.pos_ == 'NUM'])\n",
        "        num_entities = len(list(doc.ents))\n",
        "        num_spelling_errors = len(spell.unknown(essay.split()))\n",
        "        avh_word_length = np.mean([len(word) for word in essay.split()])\n",
        "        avg_sentence_length = np.mean([len(sent) for sent in list(doc.sents)])\n",
        "        return [num_sentences, num_words, num_unique_words, num_stop_words, num_proper_nouns, num_verbs, num_adjectives, num_adverbs, num_nouns, num_prepositions, num_pronouns,\n",
        "                num_conjunctions, num_interjections, num_punctuation, num_digits, num_entities, num_spelling_errors, avh_word_length, avg_sentence_length]\n",
        "\n",
        "    features = np.array([feature_creation(essay) for essay in X_train])\n",
        "\n",
        "    features_test = np.array([feature_creation(essay) for essay in X_test])\n",
        "\n",
        "    min_score = min(y_train.min(), y_test.min())\n",
        "    max_score = max(y_train.max(), y_test.max())\n",
        "\n",
        "    binned_y = pd.qcut(y_train, q=5, duplicates='drop', labels=False)\n",
        "    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    oof_preds = np.zeros(len(y_train))\n",
        "    y_train_values = np.zeros(len(y_train))\n",
        "    y_preds = []\n",
        "\n",
        "    param_grid = {\n",
        "        'n_estimators': [100, 300, 500],\n",
        "        'max_depth': [None, 10, 20, 50],\n",
        "        'min_samples_split': [2, 5, 7],\n",
        "        'max_features': ['sqrt', 'log2']\n",
        "    }\n",
        "\n",
        "    grid_search = GridSearchCV(\n",
        "            RandomForestRegressor(random_state=42),\n",
        "            param_grid,\n",
        "            cv=3,\n",
        "            n_jobs=-1,\n",
        "            scoring='neg_mean_squared_error'\n",
        "        )\n",
        "    grid_search.fit(features, y_train)\n",
        "    print('Best Parameters:', grid_search.best_params_)\n",
        "\n",
        "    best_model = grid_search.best_estimator_\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    for train_idx, val_idx in kf.split(X_train, binned_y):\n",
        "        X_tr, X_val = features[train_idx], features[val_idx]\n",
        "        y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
        "\n",
        "        model = clone(best_model)\n",
        "        model.fit(X_tr, y_tr)\n",
        "\n",
        "        y_pred = model.predict(X_val)\n",
        "        y_pred = np.clip(np.rint(y_pred), min_score, max_score).astype(int)\n",
        "        oof_preds[val_idx] = y_pred\n",
        "        y_train_values[val_idx] = y_val\n",
        "\n",
        "\n",
        "\n",
        "        y_pred_test = model.predict(features_test)\n",
        "        y_pred_test_rounded = np.clip(np.rint(y_pred_test), min_score, max_score).astype(int)\n",
        "        y_preds.append(y_pred_test_rounded)\n",
        "\n",
        "\n",
        "\n",
        "    avg_test_preds = np.mean(y_preds, axis=0)\n",
        "    final_preds_rounded = np.clip(np.rint(avg_test_preds), min_score, max_score).astype(int)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    return oof_preds, final_preds_rounded, avg_test_preds, y_train_values, y_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrdS27DoT1nL"
      },
      "source": [
        "### Creating the stacking model itself for the regression problem, with the Ridge Regression meta-learner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UZB-cqULiN-c"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from sklearn.linear_model import RidgeCV\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "oof_preds_transformer, preds_test_transformer, preds_test_k_fold_avg_bert, bert_y_train, bert_y_test = generate_stacking_preds_time('bert-base-uncased', X_train_all, X_test_all, y_train_all, y_test_all, lr = 0.00005, bs=8, epochs=80)\n",
        "bert_completion_time = time.time()\n",
        "print(f'BERT took {bert_completion_time - start_time} seconds')\n",
        "\n",
        "\n",
        "oof_preds_lstm, preds_test_lstm, preds_test_k_fold_avg_lstm, lstm_y_train, lstm_y_test = lstm_model_time(X_train_all, X_test_all, y_train_all, y_test_all)\n",
        "lstm_completion_time = time.time()\n",
        "print(f'LSTM took {lstm_completion_time - bert_completion_time} seconds')\n",
        "\n",
        "oof_preds_rf, y_pred_rf, preds_test_k_fold_avg_rf, y_train_rf, y_test_rf = random_forest_model_with_gridsearch_time(X_train_all, X_test_all, y_train_all, y_test_all)\n",
        "rf_completion_time = time.time()\n",
        "print(f'RF took {rf_completion_time - lstm_completion_time} seconds')\n",
        "\n",
        "\n",
        "X_meta_train = np.vstack((oof_preds_transformer, oof_preds_lstm, oof_preds_rf)).T\n",
        "X_meta_test = np.vstack((preds_test_k_fold_avg_bert,preds_test_k_fold_avg_lstm, preds_test_k_fold_avg_rf)).T\n",
        "\n",
        "y_meta_train = np.array(y_train_all)\n",
        "y_meta_test = np.array(y_test_all)\n",
        "\n",
        "\n",
        "ridge_cv = RidgeCV(alphas=[0.1, 1.0, 10.0, 50.0, 100.0], cv=5)\n",
        "ridge_cv.fit(X_meta_train, y_meta_train)\n",
        "\n",
        "\n",
        "meta_preds = ridge_cv.predict(X_meta_test)\n",
        "\n",
        "min_score = min(y_meta_train.min(), y_meta_test.min())\n",
        "max_score = max(y_meta_train.max(), y_meta_test.max())\n",
        "\n",
        "meta_preds_rounded = np.clip(np.rint(meta_preds), min_score, max_score).astype(int)\n",
        "\n",
        "\n",
        "qwk_meta = cohen_kappa_score(y_meta_test, meta_preds_rounded, weights='quadratic')\n",
        "print(qwk_meta)\n",
        "print(f'QWK score: {qwk_meta}')\n",
        "print(f'Alpha: {ridge_cv.alpha_}')\n",
        "print(f'Coefficients: {ridge_cv.coef_}')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "stacking_completion_time = time.time()\n",
        "print(f'Stacking took {stacking_completion_time - start_time} seconds')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GwQFo4lUB3X"
      },
      "source": [
        "# Creating the stacking set up for classification models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayeaf0CjUWj6"
      },
      "source": [
        "### Creating the BERT-based model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EcT7oQPRDmWq"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "from sklearn.metrics import cohen_kappa_score, accuracy_score\n",
        "import optuna\n",
        "from torch.nn.functional import softmax\n",
        "\n",
        "# Prompt 1 Best Params:: {'lr': 4e-05, 'batch_size': 32}\n",
        "# Prompt 2 Best Params: {'lr': 4e-05, 'batch_size': 8}\n",
        "# Prompt 3 Best Params: {'lr': 4e-05, 'batch_size': 8}\n",
        "# Prompt 4 Best Params: {'lr': 4e-05, 'batch_size': 8}\n",
        "# Prompt 5 Best Params: {'lr': 5e-05, 'batch_size': 8}\n",
        "# Prompt 6 Best Params: {'lr': 5e-05, 'batch_size': 8}\n",
        "# Prompt 7 Best Params: {'lr': 3e-05, 'batch_size': 8}\n",
        "# Prompt 8 Best Params: {'lr': 5e-05, 'batch_size': 8}\n",
        "def generate_stacking_preds_classification_time(model_name, X_train, X_test, y_train, y_test, lr = 0.00001, bs = 16, epochs = 30):\n",
        "  class EssayDatasetClassification(Dataset):\n",
        "        def __init__(self, encodings, labels, seq_len = 512):\n",
        "            self.encodings = encodings\n",
        "            self.labels = labels\n",
        "            self.seq_len = 512\n",
        "\n",
        "        def __getitem__(self, item):\n",
        "            return {\n",
        "                'input_ids': self.encodings['input_ids'][item].clone().detach(),\n",
        "                'attention_mask': self.encodings['attention_mask'][item].clone().detach(),\n",
        "                'labels': torch.tensor(self.labels[item], dtype=torch.long)\n",
        "            }\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self.labels)\n",
        "\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "  X_train = X_train.reset_index(drop=True)\n",
        "  y_train = y_train.reset_index(drop=True)\n",
        "\n",
        "  X_test = X_test.reset_index(drop=True)\n",
        "  y_test = y_test.reset_index(drop=True)\n",
        "\n",
        "  unique_scores = sorted(list(set(np.concatenate([np.array(y_train), np.array(y_test)]))))\n",
        "  score_to_class = {score: i for i, score in enumerate(unique_scores)}\n",
        "  class_to_score = {i: score for score, i in score_to_class.items()}\n",
        "  num_classes = len(score_to_class)\n",
        "\n",
        "  y_train = pd.Series([score_to_class[s] for s in y_train]).reset_index(drop=True)\n",
        "  y_test = pd.Series([score_to_class[s] for s in y_test]).reset_index(drop=True)\n",
        "\n",
        "  min_score = min(min(y_train), min(y_test))\n",
        "  max_score = max(max(y_train), max(y_test))\n",
        "\n",
        "\n",
        "\n",
        "  def make_compute_metrics(min_score, max_score):\n",
        "    def compute_metrics(eval_pred):\n",
        "        logits, labels = eval_pred\n",
        "        preds = np.argmax(logits, axis=-1)\n",
        "        true_scores = [class_to_score[i] for i in labels]\n",
        "        pred_scores = [class_to_score[i] for i in preds]\n",
        "        qwk = cohen_kappa_score(true_scores, pred_scores, weights='quadratic')\n",
        "        acc = accuracy_score(true_scores, pred_scores)\n",
        "        return {'eval_qwk': qwk, 'accuracy': acc}\n",
        "    return compute_metrics\n",
        "\n",
        "  def tokenize_texts(text):\n",
        "      cleaned_texts = []\n",
        "      for t in text:\n",
        "        t = t.replace('\\n', ' ').replace('\\r', ' ').replace('\\t', ' ')\n",
        "        t = re.sub(r'\\s+', ' ', t)\n",
        "        t = t.strip()\n",
        "        cleaned_texts.append(t)\n",
        "\n",
        "      return tokenizer(cleaned_texts, padding='max_length', truncation=True, max_length=512, return_tensors='pt')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  binned_y = pd.qcut(y_train, q=5, duplicates='drop', labels = False)\n",
        "  kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "  preds = np.zeros((len(y_train), num_classes))\n",
        "  test_preds = []\n",
        "\n",
        "  for train_idx, val_idx in kf.split(X_train, binned_y):\n",
        "          model = AutoModelForSequenceClassification.from_pretrained(model_name, problem_type='single_label_classification', num_labels = num_classes)\n",
        "\n",
        "          train_data = X_train.iloc[train_idx].tolist()\n",
        "          val_data = X_train.iloc[val_idx].tolist()\n",
        "          train_labels = y_train.iloc[train_idx].tolist()\n",
        "          val_labels = y_train.iloc[val_idx].tolist()\n",
        "\n",
        "          train_encoding = tokenize_texts(train_data)\n",
        "          val_encoding = tokenize_texts(val_data)\n",
        "\n",
        "          test_encoding = tokenize_texts(X_test.tolist())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "          train_dataset = EssayDatasetClassification(train_encoding, train_labels)\n",
        "          test_dataset = EssayDatasetClassification(test_encoding, y_test)\n",
        "          val_dataset = EssayDatasetClassification(val_encoding, val_labels)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "          for name, param in model.named_parameters():\n",
        "              param.requires_grad = False\n",
        "\n",
        "          for name, param in model.named_parameters():\n",
        "            if any(layer in name for layer in ['encoder.layer.8', 'encoder.layer.9','encoder.layer.10', 'encoder.layer.11', 'pooler', 'classifier']):\n",
        "                param.requires_grad = True\n",
        "\n",
        "          # min_score = min(y_train)\n",
        "          # max_score = max(y_train)\n",
        "\n",
        "\n",
        "          training_args = TrainingArguments(\n",
        "            output_dir='./results',\n",
        "            learning_rate=lr,\n",
        "            per_device_train_batch_size=bs,\n",
        "            per_device_eval_batch_size=bs,\n",
        "            num_train_epochs= epochs,\n",
        "            logging_strategy = 'epoch',\n",
        "            eval_strategy = 'epoch',\n",
        "            save_strategy = 'epoch',\n",
        "            load_best_model_at_end = True,\n",
        "            metric_for_best_model = 'eval_qwk',\n",
        "            greater_is_better=True,\n",
        "            report_to = 'none',\n",
        "\n",
        "\n",
        "        )\n",
        "\n",
        "      #   train_dataset = EssayDatasetRegression(train_encoding, y_train)\n",
        "      #   test_dataset = EssayDatasetRegression(test_encoding, y_test)\n",
        "      #   val_dataset = EssayDatasetRegression(val_encoding, y_val)\n",
        "\n",
        "          trainer = Trainer(\n",
        "              model=model,\n",
        "              args=training_args,\n",
        "              train_dataset=train_dataset,\n",
        "              eval_dataset=val_dataset,\n",
        "              compute_metrics=make_compute_metrics(min_score, max_score),\n",
        "              callbacks = [EarlyStoppingCallback(early_stopping_patience=3)]\n",
        "          )\n",
        "\n",
        "          trainer.train()\n",
        "\n",
        "          results = trainer.predict(val_dataset)\n",
        "          val_preds = np.argmax(results.predictions, axis=1)\n",
        "          val_probs = softmax(torch.tensor(results.predictions), dim=1).numpy()\n",
        "          val_pred_scores = [class_to_score[i] for i in val_preds.astype(int)]\n",
        "          val_true_scores = [class_to_score[i] for i in results.label_ids]\n",
        "          preds[val_idx] = val_probs\n",
        "\n",
        "\n",
        "\n",
        "          results_test = trainer.predict(test_dataset)\n",
        "          test_preds_2 = np.argmax(results_test.predictions, axis=1)\n",
        "          test_probs = softmax(torch.tensor(results_test.predictions), dim=1).numpy()\n",
        "          test_pred_scores = [class_to_score[i] for i in test_preds_2.astype(int)]\n",
        "          true_test_scores = [class_to_score[i] for i in results_test.label_ids]\n",
        "          test_preds.append(test_probs)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  avg_test_preds = np.mean(test_preds, axis=0)\n",
        "  final_test_preds_scores1 = np.argmax(avg_test_preds, axis=1)\n",
        "  final_test_preds_rounded = [class_to_score[i] for i in final_test_preds_scores1]\n",
        "  final_true_test_scores = [class_to_score[i] for i in y_test]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  return preds, final_test_preds_rounded, avg_test_preds, y_train, y_test\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLSwGlKiUwoR"
      },
      "source": [
        "### Creating the LSTM-based model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UYkYC3dUD4pf"
      },
      "outputs": [],
      "source": [
        "import gensim.downloader as api\n",
        "from gensim.models import KeyedVectors\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from collections import Counter\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "import optuna\n",
        "import torch.nn.functional as F\n",
        "# Prompt 1 optim params: {'hidden_dim': 256, 'lr': 0.00040147129315448076, 'weight_decay': 4.194966656572804e-05, 'dropout1': 0.48972751622541333, 'dropout2': 0.5637289114635329, 'dropout3': 0.4006279296851355, 'batch_size': 64}\n",
        "# Prompt 2 Best Params: {'hidden_dim': 128, 'lr': 0.0008329626514872093, 'weight_decay': 1.10904162552094e-06, 'dropout1': 0.4113129372112616, 'dropout2': 0.5339519738927687, 'dropout3': 0.6474745827693765, 'batch_size': 32}\n",
        "# Prompt 3 Best Params: {'hidden_dim': 64, 'lr': 0.0009176211868710996, 'weight_decay': 8.47468900298905e-05, 'dropout1': 0.4402196829382994, 'dropout2': 0.6543812752462479, 'dropout3': 0.30572393075878385, 'batch_size': 128}\n",
        "# Prompt 4 Best Params: Best trial: {'hidden_dim': 256, 'lr': 0.0006392739979334983, 'weight_decay': 1.0246979767749631e-06, 'dropout1': 0.6331948007619779, 'dropout2': 0.3024115352348601, 'dropout3': 0.4826430525213473, 'batch_size': 64}\n",
        "# Prompt 5 Best Params: {'hidden_dim': 256, 'lr': 0.0007888666643834901, 'weight_decay': 1.9973208515938436e-05, 'dropout1': 0.5150492631843749, 'dropout2': 0.6836007071470228, 'dropout3': 0.44931934750457436, 'batch_size': 64}\n",
        "# Prompt 6 Best Params: {'hidden_dim': 256, 'lr': 0.0004221715369392723, 'weight_decay': 1.6693933987910344e-06, 'dropout1': 0.642989336447883, 'dropout2': 0.6149595736926003, 'dropout3': 0.4561023529421714, 'batch_size': 16}\n",
        "# Prompt 7 Best Params: {'hidden_dim': 128, 'lr': 0.0007031295542079031, 'weight_decay': 1.1167323499868158e-05, 'dropout1': 0.43014203679075136, 'dropout2': 0.6072345773651249, 'dropout3': 0.5301384805467484, 'batch_size': 64}\n",
        "# Prompt 8 Best Params: {'hidden_dim': 256, 'lr': 0.00044054250769868446, 'weight_decay': 1.8209940215624457e-05, 'dropout1': 0.6820871115494689, 'dropout2': 0.5530971947983455, 'dropout3': 0.6417406365573428, 'batch_size': 64}\n",
        "def lstm_model_classification_time(X_train, X_test, y_train, y_test):\n",
        "    w2v_model = api.load('word2vec-google-news-300')\n",
        "    embedding_size = w2v_model.vector_size\n",
        "\n",
        "    X_train = X_train.reset_index(drop=True)\n",
        "    y_train = y_train.reset_index(drop=True)\n",
        "\n",
        "    X_test = X_test.reset_index(drop=True)\n",
        "    y_test = y_test.reset_index(drop=True)\n",
        "\n",
        "    unique_scores = sorted(list(set(np.concatenate([np.array(y_train), np.array(y_test)]))))\n",
        "    score_to_class = {score: i for i, score in enumerate(unique_scores)}\n",
        "    class_to_score = {i: score for score, i in score_to_class.items()}\n",
        "    num_classes = len(score_to_class)\n",
        "\n",
        "    y_train = pd.Series([score_to_class[s] for s in y_train])\n",
        "    y_test = pd.Series([score_to_class[s] for s in y_test])\n",
        "\n",
        "    y_train = y_train.reset_index(drop=True)\n",
        "    y_test = y_test.reset_index(drop=True)\n",
        "\n",
        "    min_score = min(min(y_train), min(y_test))\n",
        "    max_score = max(max(y_train), max(y_test))\n",
        "\n",
        "\n",
        "    class Vocab:\n",
        "        def __init__(self, token_freqs, min_freq=1, specials=['<pad>', '<unk>']):\n",
        "            self.itos = list(specials)\n",
        "            self.stoi = {tok: i for i, tok in enumerate(self.itos)}\n",
        "            for token, freq in token_freqs.items():\n",
        "                if freq >= min_freq and token not in self.stoi:\n",
        "                    self.stoi[token] = len(self.itos)\n",
        "                    self.itos.append(token)\n",
        "        def __len__(self):\n",
        "            return len(self.itos)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    class EssayDataset(Dataset):\n",
        "        def __init__(self, essays, labels, max_len, vocab):\n",
        "            self.essays = essays\n",
        "            self.labels = labels\n",
        "            self.max_length = max_len\n",
        "            self.vocab = vocab\n",
        "\n",
        "        def create_encodings(self, text):\n",
        "            word_token = word_tokenize(text.lower())\n",
        "            input_ids = [self.vocab.stoi.get(word, self.vocab.stoi['<unk>']) for word in word_token]\n",
        "            if len(input_ids) < self.max_length:\n",
        "                input_ids += [self.vocab.stoi['<pad>']] * (self.max_length - len(input_ids))\n",
        "            else:\n",
        "                input_ids = input_ids[:self.max_length]\n",
        "            return torch.tensor(input_ids)\n",
        "\n",
        "        def __getitem__(self, index):\n",
        "            essay = self.create_encodings(self.essays[index])\n",
        "            label = self.labels[index]\n",
        "            return essay, label\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self.essays)\n",
        "\n",
        "    class BiLSTM_CNN(nn.Module):\n",
        "        def __init__(self, vocab_size, embedding_dim, hidden_dim, embedding_matrix):\n",
        "            super().__init__()\n",
        "            self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
        "            self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
        "            self.conv1 = nn.Conv1d(hidden_dim * 2, hidden_dim, kernel_size=3, padding=1)\n",
        "            self.dropout1 = nn.Dropout(0.642989336447883)\n",
        "            self.conv2 = nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, padding=1)\n",
        "            self.dropout2 = nn.Dropout(0.6149595736926003)\n",
        "            self.pool = nn.AdaptiveAvgPool1d(1)\n",
        "            self.dropout3 = nn.Dropout(0.4561023529421714)\n",
        "            self.fc = nn.Linear(hidden_dim, num_classes)\n",
        "\n",
        "        def forward(self, x):\n",
        "            embedded = self.embedding(x)\n",
        "            lstm_out, _ = self.lstm(embedded)\n",
        "            conv1_out = torch.relu(self.conv1(lstm_out.permute(0, 2, 1)))\n",
        "            conv2_out = torch.relu(self.conv2(conv1_out))\n",
        "            pool_out = self.pool(conv2_out).squeeze(2)\n",
        "            output = self.fc(pool_out)\n",
        "            return output\n",
        "\n",
        "    def cleaning_data(text):\n",
        "        text = text.lower()\n",
        "        text = text.replace('\\n', ' ')\n",
        "        text = text.replace('\\t', ' ')\n",
        "        text = text.replace('\\r', ' ')\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        text = text.strip()\n",
        "        return text\n",
        "\n",
        "    essay_data = X_train.tolist() + X_test.tolist()\n",
        "    essays = [cleaning_data(essay) for essay in essay_data]\n",
        "    X_train = pd.Series(essays[:len(X_train)])\n",
        "    X_test = pd.Series(essays[len(X_train):])\n",
        "\n",
        "\n",
        "\n",
        "    counter = Counter()\n",
        "    for essay in essays:\n",
        "        counter.update(word_tokenize(essay.lower()))\n",
        "    most_common = counter.most_common(4000)\n",
        "    vocab = Vocab(dict(most_common), min_freq=1)\n",
        "\n",
        "    embedding_matrix = np.zeros((len(vocab), embedding_size))\n",
        "    for i, word in enumerate(vocab.itos):\n",
        "        embedding_matrix[i] = w2v_model[word] if word in w2v_model else np.random.normal(scale=0.6, size=(embedding_size,))\n",
        "    embedding_matrix = torch.tensor(embedding_matrix, dtype=torch.float32)\n",
        "\n",
        "    min_score = int(min(y_train.min(), y_test.min()))\n",
        "    max_score = int(max(y_train.max(), y_test.max()))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    bins = pd.qcut(y_train, q=5, duplicates='drop', labels = False)\n",
        "    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    oof_preds = np.zeros((len(y_train), num_classes))\n",
        "    y_train_values = np.zeros(len(y_train))\n",
        "    test_preds = []\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    for train_idx, val_idx in kf.split(X_train, bins):\n",
        "        X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
        "        y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
        "\n",
        "        train_dataset = EssayDataset(X_tr.tolist(), y_tr.tolist(), 512, vocab)\n",
        "        val_dataset = EssayDataset(X_val.tolist(), y_val.tolist(), 512, vocab)\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "        model = BiLSTM_CNN(len(vocab), embedding_size, 256, embedding_matrix)\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr = 0.0004221715369392723, weight_decay = 1.6693933987910344e-06)\n",
        "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer,\n",
        "        mode='max',\n",
        "        factor=0.5,\n",
        "        patience=1\n",
        "      )\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        for epoch in range(10):\n",
        "            model.train()\n",
        "            total_loss = 0\n",
        "            for essays, labels in train_loader:\n",
        "                optimizer.zero_grad()\n",
        "                output = model(essays)\n",
        "                loss = criterion(output, labels.long())\n",
        "                total_loss += loss.item()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "            print(f'Epoch {epoch + 1}, Loss: {total_loss / len(train_loader)}')\n",
        "\n",
        "        model.eval()\n",
        "        val_preds = []\n",
        "        val_probs = []\n",
        "        with torch.no_grad():\n",
        "            for essays, _ in val_loader:\n",
        "                output = model(essays).cpu().numpy()\n",
        "                probs = F.softmax(torch.tensor(output), dim=1).numpy()\n",
        "\n",
        "                val_probs.append(probs)\n",
        "                val_preds.extend(np.argmax(probs, axis=1))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        val_probs = np.concatenate(val_probs, axis=0)\n",
        "        val_pred_scores = [class_to_score[i] for i in val_preds]\n",
        "        val_true_scores = [class_to_score[i] for i in y_val.tolist()]\n",
        "        oof_preds[val_idx] = val_probs\n",
        "\n",
        "        y_train_values[val_idx] = val_true_scores\n",
        "        qwk = cohen_kappa_score(val_pred_scores, val_true_scores, weights='quadratic')\n",
        "\n",
        "        scheduler.step(qwk)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        test_dataset = EssayDataset(X_test.tolist(), np.zeros(len(X_test)), 512, vocab)\n",
        "        test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "        model.eval()\n",
        "        fold_test_preds = []\n",
        "        fold_test_probs = []\n",
        "        with torch.no_grad():\n",
        "            for essays, _ in test_loader:\n",
        "                output = model(essays).cpu().numpy()\n",
        "                fold_test_probs.append(F.softmax(torch.tensor(output), dim=1).numpy())\n",
        "\n",
        "\n",
        "        test_preds.append(np.concatenate(fold_test_probs, axis=0))\n",
        "\n",
        "\n",
        "    avg_test_probs = np.mean(test_preds, axis=0)\n",
        "    final_test_preds = np.argmax(avg_test_probs, axis=1)\n",
        "    final_test_preds = [class_to_score[i] for i in final_test_preds]\n",
        "\n",
        "    true_scores = [class_to_score[i] for i in y_test.tolist()]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    return oof_preds, final_test_preds, avg_test_probs, y_train_values, y_test\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aiitQOw0U78L"
      },
      "source": [
        "### Creating the Random Forest model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QWILJsfWENgi"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from sklearn.base import clone\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import spacy\n",
        "from spellchecker import SpellChecker\n",
        "\n",
        "def random_forest_model_with_gridsearch_classification_time(X_train, X_test, y_train, y_test):\n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "    spell = SpellChecker()\n",
        "\n",
        "    def cleaning_data(text):\n",
        "      text = text.replace('\\n', ' ')\n",
        "      text = text.replace('\\t', ' ')\n",
        "      text = text.replace('\\r', ' ')\n",
        "      text = re.sub(r'\\s+', ' ', text)\n",
        "      text = text.strip()\n",
        "      return text\n",
        "\n",
        "    X_train = X_train.apply(cleaning_data)\n",
        "    X_test = X_test.apply(cleaning_data)\n",
        "\n",
        "    def feature_creation(essay):\n",
        "        doc = nlp(essay)\n",
        "        num_sentences = len(list(doc.sents))\n",
        "        num_words = len(essay.split())\n",
        "        num_unique_words = len(set(essay.split()))\n",
        "        num_stop_words = len([token for token in doc if token.is_stop])\n",
        "        num_proper_nouns = len([token for token in doc if token.pos_ == 'PROPN'])\n",
        "        num_verbs = len([token for token in doc if token.pos_ == 'VERB'])\n",
        "        num_adjectives = len([token for token in doc if token.pos_ == 'ADJ'])\n",
        "        num_adverbs = len([token for token in doc if token.pos_ == 'ADV'])\n",
        "        num_nouns = len([token for token in doc if token.pos_ == 'NOUN'])\n",
        "        num_prepositions = len([token for token in doc if token.pos_ == 'ADP'])\n",
        "        num_pronouns = len([token for token in doc if token.pos_ == 'PRON'])\n",
        "        num_conjunctions = len([token for token in doc if token.pos_ == 'CCONJ'])\n",
        "        num_interjections = len([token for token in doc if token.pos_ == 'INTJ'])\n",
        "        num_punctuation = len([token for token in doc if token.pos_ == 'PUNCT'])\n",
        "        num_digits = len([token for token in doc if token.pos_ == 'NUM'])\n",
        "        num_entities = len(list(doc.ents))\n",
        "        num_spelling_errors = len(spell.unknown(essay.split()))\n",
        "        avg_word_length = np.mean([len(word) for word in essay.split()])\n",
        "        avg_sentence_length = np.mean([len(sent) for sent in list(doc.sents)])\n",
        "        return [\n",
        "            num_sentences, num_words, num_unique_words, num_stop_words, num_proper_nouns,\n",
        "            num_verbs, num_adjectives, num_adverbs, num_nouns, num_prepositions,\n",
        "            num_pronouns, num_conjunctions, num_interjections, num_punctuation, num_digits,\n",
        "            num_entities, num_spelling_errors, avg_word_length, avg_sentence_length\n",
        "        ]\n",
        "\n",
        "    features = np.array([feature_creation(essay) for essay in X_train])\n",
        "    features_test = np.array([feature_creation(essay) for essay in X_test])\n",
        "\n",
        "    unique_scores = sorted(list(set(np.concatenate([np.array(y_train), np.array(y_test)]))))\n",
        "    score_to_class = {score: i for i, score in enumerate(unique_scores)}\n",
        "    class_to_score = {i: score for score, i in score_to_class.items()}\n",
        "\n",
        "    y_train_class = y_train.map(score_to_class)\n",
        "    y_test_class = y_test.map(score_to_class)\n",
        "\n",
        "    binned_y = pd.qcut(y_train_class, q=5, duplicates='drop', labels=False)\n",
        "    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "    oof_preds = np.zeros((len(y_train), len(score_to_class)))\n",
        "    y_train_values = np.zeros(len(y_train))\n",
        "    test_probs_folds = []\n",
        "\n",
        "    param_grid = {\n",
        "        'n_estimators': [100, 300, 500],\n",
        "        'max_depth': [None, 10, 20, 50],\n",
        "        'min_samples_split': [2, 5, 7],\n",
        "        'max_features': ['sqrt', 'log2']\n",
        "    }\n",
        "\n",
        "    grid_search = GridSearchCV(\n",
        "        RandomForestClassifier(random_state=42),\n",
        "        param_grid,\n",
        "        cv=3,\n",
        "        n_jobs=-1,\n",
        "        scoring='accuracy'\n",
        "    )\n",
        "    grid_search.fit(features, y_train_class)\n",
        "    print('Best Parameters:', grid_search.best_params_)\n",
        "\n",
        "    best_model = grid_search.best_estimator_\n",
        "\n",
        "    for train_idx, val_idx in kf.split(X_train, binned_y):\n",
        "        X_tr, X_val = features[train_idx], features[val_idx]\n",
        "        y_tr, y_val = y_train_class.iloc[train_idx], y_train_class.iloc[val_idx]\n",
        "\n",
        "        model = clone(best_model)\n",
        "        model.fit(X_tr, y_tr)\n",
        "\n",
        "        val_probs = model.predict_proba(X_val)\n",
        "        val_preds = np.argmax(val_probs, axis=1)\n",
        "\n",
        "        oof_preds[val_idx] = val_probs\n",
        "        y_train_values[val_idx] = [class_to_score[i] for i in y_val]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        test_probs_folds.append(model.predict_proba(features_test))\n",
        "\n",
        "\n",
        "    avg_test_probs = np.mean(test_probs_folds, axis=0)\n",
        "    final_test_class_preds = np.argmax(avg_test_probs, axis=1)\n",
        "    final_test_scores = [class_to_score[i] for i in final_test_class_preds]\n",
        "    true_test_scores = [class_to_score[i] for i in y_test_class]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    return oof_preds, final_test_scores, avg_test_probs, y_train_values, y_test.tolist()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODqgGrE9VNks"
      },
      "source": [
        "### Creating the stacking model itself for the classification problem, with the Logistic Regression meta-learner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N8hPjNVjEfpz"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from sklearn.linear_model import RidgeCV\n",
        "\n",
        "\n",
        "start_time = time.time()\n",
        "X_train_all = pd.Series(X_train_all)\n",
        "X_test_all = pd.Series(X_test_all)\n",
        "y_train_all = pd.Series(y_train_all)\n",
        "y_test_all = pd.Series(y_test_all)\n",
        "\n",
        "\n",
        "oof_preds_transformer, preds_test_transformer, preds_test_k_fold_avg_bert, bert_y_train, bert_y_test = generate_stacking_preds_classification_time('bert-base-uncased', X_train_all, X_test_all, y_train_all, y_test_all, lr = 0.00005, bs=8, epochs=30)\n",
        "bert_completion_time = time.time()\n",
        "print(f'BERT took {bert_completion_time - start_time} seconds')\n",
        "\n",
        "\n",
        "oof_preds_lstm, preds_test_lstm, preds_test_k_fold_avg_lstm, lstm_y_train, lstm_y_test = lstm_model_classification_time(X_train_all, X_test_all, y_train_all, y_test_all)\n",
        "lstm_completion_time = time.time()\n",
        "print(f'LSTM took {lstm_completion_time - bert_completion_time} seconds')\n",
        "\n",
        "oof_preds_rf, y_pred_rf, preds_test_k_fold_avg_rf, y_train_rf, y_test_rf = random_forest_model_with_gridsearch_classification_time(X_train_all, X_test_all, y_train_all, y_test_all)\n",
        "rf_completion_time = time.time()\n",
        "print(f'RF took {rf_completion_time - lstm_completion_time} seconds')\n",
        "\n",
        "from sklearn.linear_model import RidgeCV\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "\n",
        "\n",
        "X_meta_train = np.concatenate([oof_preds_transformer, oof_preds_lstm, oof_preds_rf], axis=1)\n",
        "X_meta_test = np.concatenate([preds_test_k_fold_avg_bert, preds_test_k_fold_avg_lstm, preds_test_k_fold_avg_rf], axis=1)\n",
        "\n",
        "\n",
        "unique_scores = sorted(list(set(np.concatenate([np.array(y_train_all), np.array(y_test_all)]))))\n",
        "score_to_class = {score: i for i, score in enumerate(unique_scores)}\n",
        "class_to_score = {i: score for score, i in score_to_class.items()}\n",
        "\n",
        "y_train_all = [score_to_class[s] for s in y_train_all]\n",
        "y_test_all = [score_to_class[s] for s in y_test_all]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "meta_model = LogisticRegressionCV(max_iter=10000, cv=5)\n",
        "meta_model.fit(X_meta_train, y_train_all)\n",
        "meta_preds = meta_model.predict(X_meta_test)\n",
        "\n",
        "min_class = min(score_to_class.values())\n",
        "max_class = max(score_to_class.values())\n",
        "meta_class_preds = np.clip(np.rint(meta_preds), min_class, max_class).astype(int)\n",
        "\n",
        "\n",
        "meta_score_preds = [class_to_score[i] for i in meta_class_preds]\n",
        "true_scores = [class_to_score[i] for i in y_test_all]\n",
        "\n",
        "\n",
        "qwk_meta = cohen_kappa_score(true_scores, meta_score_preds, weights='quadratic')\n",
        "print(f'QWK: {qwk_meta}')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "stacking_completion_time = time.time()\n",
        "print(f'Stacking took {stacking_completion_time - start_time} seconds')\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDIVIbdsyzlp"
      },
      "source": [
        "# AI Detection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06BL6qtRVuPX"
      },
      "source": [
        "### Importing data and performing EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ogeEHXQsL7A6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "data_2 = pd.read_csv('/content/drive/MyDrive/AI-Detection/train_v2_drcat_02.csv')\n",
        "data_2 = data_2[['text', 'label']]\n",
        "zero_label_count = data_2[data_2['label'] == 0].shape[0]\n",
        "one_label_count = data_2[data_2['label'] == 1].shape[0]\n",
        "\n",
        "print(f'Number of rows with label 0: {zero_label_count}')\n",
        "print(f'Number of rows with label 1: {one_label_count}')\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.bar(x=['Human text', 'AI-generated text'], height=[zero_label_count, one_label_count])\n",
        "plt.ylabel('Number of Samples')\n",
        "plt.title('Class Distribution')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "class_weights = compute_class_weight(class_weight='balanced', classes=np.array([0, 1]), y=data_2['label'])\n",
        "print(class_weights)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIaRQjyDV-n7"
      },
      "source": [
        "### Creating an even split of the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5gaja6xaE3C"
      },
      "outputs": [],
      "source": [
        "sample_size_per_class = 5000\n",
        "\n",
        "\n",
        "sampled_df = (\n",
        "    data_2\n",
        "    .groupby('label', group_keys=False)\n",
        "    .apply(lambda x: x.sample(min(len(x), sample_size_per_class), random_state=42))\n",
        "    .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "\n",
        "print(sampled_df['label'].value_counts())\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AyN5N2dbWGfC"
      },
      "source": [
        "### Train Test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hWbo6lk8WTLE"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(sampled_df['text'], sampled_df['label'], test_size=0.2, random_state=42, stratify=sampled_df['label'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbckT-m_WQZ-"
      },
      "source": [
        "### Creating BERT-based model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YYMRiq69WOyN"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "from sklearn.metrics import cohen_kappa_score, accuracy_score, recall_score, precision_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import optuna\n",
        "from torch.nn.functional import softmax\n",
        "import re\n",
        "\n",
        "\n",
        "# Best Params: {'lr': 2e-05, 'batch_size': 32}\n",
        "def generate_stacking_preds_classification_ai(model_name, X_train, X_test, y_train, y_test, lr = 0.00001, bs = 16, epochs = 30):\n",
        "  class EssayDatasetClassification(Dataset):\n",
        "        def __init__(self, encodings, labels, seq_len = 512):\n",
        "            self.encodings = encodings\n",
        "            self.labels = labels\n",
        "            self.seq_len = 512\n",
        "\n",
        "        def __getitem__(self, item):\n",
        "            return {\n",
        "                'input_ids': self.encodings['input_ids'][item].clone().detach(),\n",
        "                'attention_mask': self.encodings['attention_mask'][item].clone().detach(),\n",
        "                'labels': torch.tensor(self.labels[item], dtype=torch.long)\n",
        "            }\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self.labels)\n",
        "\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "\n",
        "  X_train = X_train.reset_index(drop=True)\n",
        "  y_train = y_train.reset_index(drop=True)\n",
        "\n",
        "  X_test = X_test.reset_index(drop=True)\n",
        "  y_test = y_test.reset_index(drop=True)\n",
        "\n",
        "  # unique_scores = sorted(list(set(np.concatenate([np.array(y_train), np.array(y_test)]))))\n",
        "  # num_classes = len(score_to_class)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def make_compute_metrics():\n",
        "    def compute_metrics(eval_pred):\n",
        "        logits, labels = eval_pred\n",
        "        preds = np.argmax(logits, axis=-1)\n",
        "        acc = accuracy_score(labels, preds)\n",
        "        recall = recall_score(labels, preds)\n",
        "        precision = precision_score(labels, preds)\n",
        "        f1 = f1_score(labels, preds)\n",
        "        return  {'accuracy': acc,\n",
        "                 'recall' : recall,\n",
        "                 'precision' : precision,\n",
        "                 'F1 score' : f1}\n",
        "    return compute_metrics\n",
        "\n",
        "  def tokenize_texts(text):\n",
        "    cleaned_texts = []\n",
        "    for t in text:\n",
        "      t = t.replace('\\n', ' ').replace('\\r', ' ').replace('\\t', ' ')\n",
        "      t = re.sub(r'\\s+', ' ', t)\n",
        "      t = t.strip()\n",
        "      cleaned_texts.append(t)\n",
        "\n",
        "    return tokenizer(cleaned_texts, padding='max_length', truncation=True, max_length=512, return_tensors='pt')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  binned_y = pd.qcut(y_train, q=5, duplicates='drop', labels = False)\n",
        "  kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "  preds = np.zeros((len(y_train), 2))\n",
        "  test_preds = []\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(X_train, y_train, test_size=0.2, random_state=42, stratify = y_train)\n",
        "\n",
        "  X_train_split = X_train_split.tolist()\n",
        "  X_val_split = X_val_split.tolist()\n",
        "  y_train_split = y_train_split.tolist()\n",
        "  y_val_split = y_val_split.tolist()\n",
        "  X_test = X_test.tolist()\n",
        "  y_test = y_test.tolist()\n",
        "\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "  X_train_split_encoding = tokenize_texts(X_train_split)\n",
        "  X_test_encoding = tokenize_texts(X_test)\n",
        "  X_val_split_encoding = tokenize_texts(X_val_split)\n",
        "\n",
        "\n",
        "  X_train_split_dataset = EssayDatasetClassification(X_train_split_encoding, y_train_split)\n",
        "  X_test_dataset = EssayDatasetClassification(X_test_encoding, y_test)\n",
        "  X_val_split_dataset = EssayDatasetClassification(X_val_split_encoding, y_val_split)\n",
        "  # true_labels = y_test.tolist()\n",
        "\n",
        "  model2 = AutoModelForSequenceClassification.from_pretrained(model_name, problem_type='single_label_classification', num_labels = 2)\n",
        "\n",
        "  for name, param in model2.named_parameters():\n",
        "      param.requires_grad = False\n",
        "\n",
        "  for name, param in model2.named_parameters():\n",
        "      if any(layer in name for layer in ['encoder.layer.8', 'encoder.layer.9','encoder.layer.10', 'encoder.layer.11', 'pooler', 'classifier']):\n",
        "          param.requires_grad = True\n",
        "\n",
        "\n",
        "\n",
        "  training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    learning_rate=lr,\n",
        "    per_device_train_batch_size=bs,\n",
        "    per_device_eval_batch_size=bs,\n",
        "    num_train_epochs= epochs,\n",
        "    logging_strategy = 'epoch',\n",
        "    eval_strategy = 'epoch',\n",
        "    save_strategy = 'epoch',\n",
        "    load_best_model_at_end = True,\n",
        "    metric_for_best_model = 'accuracy',\n",
        "    greater_is_better=True,\n",
        "    report_to = 'none',\n",
        "\n",
        "\n",
        "  )\n",
        "\n",
        "\n",
        "  trainer2 = Trainer(\n",
        "      model=model2,\n",
        "      args=training_args,\n",
        "      train_dataset=X_train_split_dataset,\n",
        "      compute_metrics = make_compute_metrics(),\n",
        "      eval_dataset=X_val_split_dataset,\n",
        "      callbacks = [EarlyStoppingCallback(early_stopping_patience=3)]\n",
        "  )\n",
        "\n",
        "  trainer2.train()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  results_test = trainer2.predict(X_test_dataset)\n",
        "  test_preds_output = np.argmax(results_test.predictions, axis=1)\n",
        "  acc = accuracy_score(y_test, test_preds_output)\n",
        "  recall = recall_score(y_test, test_preds_output)\n",
        "  precision = precision_score(y_test, test_preds_output)\n",
        "  f1 = f1_score(y_test, test_preds_output)\n",
        "  print(f'Accuracy: {acc}')\n",
        "  print(f'Recall: {recall}')\n",
        "  print(f'Precision: {precision}')\n",
        "  print(f'F1 Score: {f1}')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#   ### This code below is for hyper-parameter tuning\n",
        "\n",
        "\n",
        "def generate_stacking_preds_hp_tune_ai(model_name, X_train, X_test, y_train, y_test, lr = 0.00001, bs = 16, epochs = 30):\n",
        "  class EssayDatasetClassification(Dataset):\n",
        "        def __init__(self, encodings, labels, seq_len = 512):\n",
        "            self.encodings = encodings\n",
        "            self.labels = labels\n",
        "            self.seq_len = 512\n",
        "\n",
        "        def __getitem__(self, item):\n",
        "            return {\n",
        "                'input_ids': self.encodings['input_ids'][item].clone().detach(),\n",
        "                'attention_mask': self.encodings['attention_mask'][item].clone().detach(),\n",
        "                'labels': torch.tensor(self.labels[item], dtype=torch.long)\n",
        "            }\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self.labels)\n",
        "\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "\n",
        "  X_train = X_train.reset_index(drop=True)\n",
        "  y_train = y_train.reset_index(drop=True)\n",
        "\n",
        "  X_test = X_test.reset_index(drop=True)\n",
        "  y_test = y_test.reset_index(drop=True)\n",
        "\n",
        "  # unique_scores = sorted(list(set(np.concatenate([np.array(y_train), np.array(y_test)]))))\n",
        "  # num_classes = len(score_to_class)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def make_compute_metrics():\n",
        "    def compute_metrics(eval_pred):\n",
        "        logits, labels = eval_pred\n",
        "        preds = np.argmax(logits, axis=-1)\n",
        "        acc = accuracy_score(labels, preds)\n",
        "        recall = recall_score(labels, preds)\n",
        "        precision = precision_score(labels, preds)\n",
        "        f1 = f1_score(labels, preds)\n",
        "        return  {'accuracy': acc,\n",
        "                 'recall' : recall,\n",
        "                 'precision' : precision,\n",
        "                 'fi_score' : f1}\n",
        "    return compute_metrics\n",
        "\n",
        "  def tokenize_texts(text):\n",
        "    cleaned_texts = []\n",
        "    for t in text:\n",
        "      t = t.replace('\\n', ' ').replace('\\r', ' ').replace('\\t', ' ')\n",
        "      t = re.sub(r'\\s+', ' ', t)\n",
        "      t = t.strip()\n",
        "      cleaned_texts.append(t)\n",
        "\n",
        "    return tokenizer(cleaned_texts, padding='max_length', truncation=True, max_length=512, return_tensors='pt')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  model = AutoModelForSequenceClassification.from_pretrained(model_name, problem_type = 'single_label_classification', num_labels = 2)\n",
        "\n",
        "  X_train_ft, X_val, y_train_ft, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42, stratify = y_train)\n",
        "\n",
        "  X_train_ft = X_train_ft.reset_index(drop=True)\n",
        "  X_val = X_val.reset_index(drop=True)\n",
        "  y_train_ft = pd.Series(y_train_ft).reset_index(drop=True)\n",
        "  y_val = pd.Series(y_val).reset_index(drop=True)\n",
        "\n",
        "\n",
        "  train_data = X_train_ft.tolist()\n",
        "  val_data = X_val.tolist()\n",
        "  train_labels = y_train_ft\n",
        "  val_labels = y_val\n",
        "\n",
        "  train_encoding = tokenize_texts(train_data)\n",
        "  val_encoding = tokenize_texts(val_data)\n",
        "\n",
        "\n",
        "  train_dataset = EssayDatasetClassification(train_encoding, train_labels)\n",
        "  val_dataset = EssayDatasetClassification(val_encoding, val_labels)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  for name, param in model.named_parameters():\n",
        "      param.requires_grad = False\n",
        "\n",
        "  for name, param in model.named_parameters():\n",
        "    if any(layer in name for layer in ['encoder.layer.8', 'encoder.layer.9','encoder.layer.10', 'encoder.layer.11', 'pooler', 'classifier']):\n",
        "        param.requires_grad = True\n",
        "\n",
        "        # min_score = min(y_train)\n",
        "        # max_score = max(y_train)\n",
        "\n",
        "\n",
        "  training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    learning_rate=lr,\n",
        "    per_device_train_batch_size=bs,\n",
        "    per_device_eval_batch_size=bs,\n",
        "    num_train_epochs= epochs,\n",
        "    logging_strategy = 'epoch',\n",
        "    eval_strategy = 'epoch',\n",
        "    save_strategy = 'epoch',\n",
        "    load_best_model_at_end = True,\n",
        "    metric_for_best_model = 'accuracy',\n",
        "    greater_is_better=True,\n",
        "    report_to = 'none',\n",
        "\n",
        "\n",
        "  )\n",
        "\n",
        "\n",
        "\n",
        "  trainer = Trainer(\n",
        "      model=model,\n",
        "      args=training_args,\n",
        "      train_dataset=train_dataset,\n",
        "      eval_dataset=val_dataset,\n",
        "      compute_metrics=make_compute_metrics(),\n",
        "      callbacks = [EarlyStoppingCallback(early_stopping_patience=3)]\n",
        "  )\n",
        "\n",
        "  trainer.train()\n",
        "\n",
        "  results = trainer.predict(val_dataset)\n",
        "  val_preds = np.argmax(results.predictions, axis=1)\n",
        "  acc = accuracy_score(results.label_ids, val_preds)\n",
        "  recall = recall_score(results.label_ids, val_preds)\n",
        "  precision = precision_score(results.label_ids, val_preds)\n",
        "  f1 = f1_score(results.label_ids, val_preds)\n",
        "  print(f'Accuracy: {acc}')\n",
        "  print(f'Recall: {recall}')\n",
        "  print(f'Precision: {precision}')\n",
        "  print(f'F1 Score: {f1}')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  return {\n",
        "    'accuracy': acc,\n",
        "    'recall': recall,\n",
        "    'precision': precision,\n",
        "    'f1': f1\n",
        "}\n",
        "\n",
        "def tune_hyperparameters(trial):\n",
        "    lr = trial.suggest_float('lr', 1e-5, 5e-5, step=1e-5)\n",
        "    bs = trial.suggest_categorical('batch_size', [8, 16, 32])\n",
        "\n",
        "    print(f'Trial {trial.number}: lr={lr}, batch_size={bs}')\n",
        "\n",
        "\n",
        "\n",
        "    metrics = generate_stacking_preds_hp_tune_ai('bert-base-uncased', X_train, X_test, y_train, y_test, lr = lr, bs = bs, epochs = 30)\n",
        "    print(f\"[Trial {trial.number}] Accuracy: {metrics['accuracy']}, \"\n",
        "          f\"Precision: {metrics['precision']}, \"\n",
        "          f\"Recall: {metrics['recall']}, \"\n",
        "          f\"F1: {metrics['f1']}\")\n",
        "\n",
        "    return metrics['accuracy']\n",
        "\n",
        "\n",
        "\n",
        "# study = optuna.create_study(direction='maximize')\n",
        "# study.optimize(tune_hyperparameters, n_trials=10)\n",
        "\n",
        "# print('Best acc:', study.best_value)\n",
        "# print('Best class weights:', study.best_trial.params)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRZF0qv6WYvI"
      },
      "source": [
        "### Checking the run time and performance for the BERT-based model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GfrnrghcYVZc"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "start_time = time.time()\n",
        "generate_stacking_preds_classification_ai('bert-base-uncased', X_train, X_test, y_train, y_test, lr = 0.00002, bs = 32, epochs = 30)\n",
        "end_time = time.time()\n",
        "print(f'Time taken: {end_time - start_time} seconds')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGzhgxWCWlWH"
      },
      "source": [
        "### Creating LSTM-based model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HPVxQT4I8dVr"
      },
      "outputs": [],
      "source": [
        "import gensim.downloader as api\n",
        "from gensim.models import KeyedVectors\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from collections import Counter\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, classification_report\n",
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "import optuna\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "import re\n",
        "from nltk import word_tokenize\n",
        "# Best Params: {'hidden_dim': 256, 'lr': 0.00014009792265935007, 'weight_decay': 4.032946922784683e-06, 'dropout1': 0.28928948977032815, 'dropout2': 0.3824601758543915, 'dropout3': 0.3210742837267551, 'batch_size': 32}\n",
        "def lstm_model_ai(X_train, X_test, y_train, y_test):\n",
        "    w2v_model = api.load('word2vec-google-news-300')\n",
        "    embedding_size = w2v_model.vector_size\n",
        "\n",
        "    X_train = X_train.reset_index(drop=True)\n",
        "    y_train = y_train.reset_index(drop=True)\n",
        "\n",
        "    X_test = X_test.reset_index(drop=True)\n",
        "    y_test = y_test.reset_index(drop=True)\n",
        "\n",
        "    # unique_scores = sorted(list(set(np.concatenate([np.array(y_train), np.array(y_test)]))))\n",
        "    # score_to_class = {score: i for i, score in enumerate(unique_scores)}\n",
        "    # class_to_score = {i: score for score, i in score_to_class.items()}\n",
        "    # num_classes = len(score_to_class)\n",
        "\n",
        "    # y_train = pd.Series([score_to_class[s] for s in y_train])\n",
        "    # y_test = pd.Series([score_to_class[s] for s in y_test])\n",
        "\n",
        "    # y_train = y_train.reset_index(drop=True)\n",
        "    # y_test = y_test.reset_index(drop=True)\n",
        "\n",
        "    # min_score = min(min(y_train), min(y_test))\n",
        "    # max_score = max(max(y_train), max(y_test))\n",
        "\n",
        "\n",
        "    class Vocab:\n",
        "        def __init__(self, token_freqs, min_freq=1, specials=['<pad>', '<unk>']):\n",
        "            self.itos = list(specials)\n",
        "            self.stoi = {tok: i for i, tok in enumerate(self.itos)}\n",
        "            for token, freq in token_freqs.items():\n",
        "                if freq >= min_freq and token not in self.stoi:\n",
        "                    self.stoi[token] = len(self.itos)\n",
        "                    self.itos.append(token)\n",
        "        def __len__(self):\n",
        "            return len(self.itos)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    class EssayDataset(Dataset):\n",
        "        def __init__(self, essays, labels, max_len, vocab):\n",
        "            self.essays = essays\n",
        "            self.labels = labels\n",
        "            self.max_length = max_len\n",
        "            self.vocab = vocab\n",
        "\n",
        "        def create_encodings(self, text):\n",
        "            word_token = word_tokenize(text)\n",
        "            input_ids = [self.vocab.stoi.get(word, self.vocab.stoi['<unk>']) for word in word_token]\n",
        "            if len(input_ids) < self.max_length:\n",
        "                input_ids += [self.vocab.stoi['<pad>']] * (self.max_length - len(input_ids))\n",
        "            else:\n",
        "                input_ids = input_ids[:self.max_length]\n",
        "            return torch.tensor(input_ids)\n",
        "\n",
        "        def __getitem__(self, index):\n",
        "            essay = self.create_encodings(self.essays[index])\n",
        "            label = float(self.labels[index])\n",
        "            return essay, torch.tensor(label)\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self.essays)\n",
        "\n",
        "    class BiLSTM_CNN(nn.Module):\n",
        "        def __init__(self, vocab_size, embedding_dim, hidden_dim, embedding_matrix):\n",
        "            super().__init__()\n",
        "            self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
        "            self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
        "            self.conv1 = nn.Conv1d(hidden_dim * 2, hidden_dim, kernel_size=3, padding=1)\n",
        "            self.dropout1 = nn.Dropout(0.28928948977032815)\n",
        "            self.conv2 = nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, padding=1)\n",
        "            self.dropout2 = nn.Dropout(0.3824601758543915)\n",
        "            self.pool = nn.AdaptiveAvgPool1d(1)\n",
        "            self.dropout3 = nn.Dropout( 0.3210742837267551)\n",
        "            self.fc = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "        def forward(self, x):\n",
        "            embedded = self.embedding(x)\n",
        "            lstm_out, _ = self.lstm(embedded)\n",
        "            conv1_out = torch.relu(self.conv1(lstm_out.permute(0, 2, 1)))\n",
        "            conv2_out = torch.relu(self.conv2(conv1_out))\n",
        "            pool_out = self.pool(conv2_out).squeeze(2)\n",
        "            output = self.fc(pool_out).squeeze(1)\n",
        "            return output\n",
        "\n",
        "    def cleaning_data(text):\n",
        "        text = text.replace('\\n', ' ')\n",
        "        text = text.replace('\\t', ' ')\n",
        "        text = text.replace('\\r', ' ')\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        text = text.strip()\n",
        "        return text\n",
        "\n",
        "    essay_data = X_train.tolist() + X_test.tolist()\n",
        "    essays = [cleaning_data(essay) for essay in essay_data]\n",
        "    X_train = pd.Series(essays[:len(X_train)])\n",
        "    X_test = pd.Series(essays[len(X_train):])\n",
        "\n",
        "\n",
        "\n",
        "    counter = Counter()\n",
        "    for essay in essays:\n",
        "        counter.update(word_tokenize(essay))\n",
        "    most_common = counter.most_common(4000)\n",
        "    vocab = Vocab(dict(most_common), min_freq=1)\n",
        "\n",
        "    embedding_matrix = np.zeros((len(vocab), embedding_size))\n",
        "    for i, word in enumerate(vocab.itos):\n",
        "        embedding_matrix[i] = w2v_model[word] if word in w2v_model else np.random.normal(scale=0.6, size=(embedding_size,))\n",
        "    embedding_matrix = torch.tensor(embedding_matrix, dtype=torch.float32)\n",
        "\n",
        "    min_score = int(min(y_train.min(), y_test.min()))\n",
        "    max_score = int(max(y_train.max(), y_test.max()))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    bins = pd.qcut(y_train, q=5, duplicates='drop', labels = False)\n",
        "    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    oof_preds = np.zeros((len(y_train), 2))\n",
        "    y_train_values = np.zeros(len(y_train))\n",
        "    test_preds = []\n",
        "    # scaler = MinMaxScaler()\n",
        "    # scaler.fit(y_train.values.reshape(-1, 1))\n",
        "\n",
        "    # Comment this out to do fine-tuning\n",
        "    # def tuning_parameters(trial):\n",
        "    #   split = int(0.8 * len(X_train))\n",
        "    #   X_tr, X_val = X_train[:split].reset_index(drop=True), X_train[split:].reset_index(drop=True)\n",
        "    #   y_tr, y_val = y_train[:split].reset_index(drop=True), y_train[split:].reset_index(drop=True)\n",
        "\n",
        "    #   hidden_dim = trial.suggest_categorical('hidden_dim', [64, 128, 256, 512])\n",
        "    #   lr = trial.suggest_float('lr', 1e-5, 1e-3, log = True)\n",
        "    #   weight_decay = trial.suggest_float('weight_decay', 1e-6, 1e-4, log=True)\n",
        "    #   dropout1 = trial.suggest_float('dropout1', 0.2, 0.8)\n",
        "    #   dropout2 = trial.suggest_float('dropout2', 0.2, 0.8)\n",
        "    #   dropout3 = trial.suggest_float('dropout3', 0.2, 0.8)\n",
        "    #   batch_size = trial.suggest_categorical('batch_size', [16, 32, 64, 128])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    #   full_train_dataset = EssayDataset(X_tr.tolist(), y_tr, 512, vocab)\n",
        "    #   full_train_loader = DataLoader(full_train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    #   full_val_dataset = EssayDataset(X_val.tolist(), y_val, 512, vocab)\n",
        "    #   full_val_loader = DataLoader(full_val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    #   model = BiLSTM_CNN(len(vocab), embedding_size, hidden_dim, embedding_matrix)\n",
        "    #   model.dropout1.p = dropout1\n",
        "    #   model.dropout2.p = dropout2\n",
        "    #   model.dropout3.p = dropout3\n",
        "    #   optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    #   criterion = nn.BCEWithLogitsLoss()\n",
        "    #   for epoch in range(5):\n",
        "    #       model.train()\n",
        "    #       total_loss = 0\n",
        "    #       for essays, labels in full_train_loader:\n",
        "    #           optimizer.zero_grad()\n",
        "    #           output = model(essays)\n",
        "    #           loss = criterion(output, labels.float())\n",
        "    #           total_loss += loss.item()\n",
        "    #           loss.backward()\n",
        "    #           optimizer.step()\n",
        "\n",
        "    #   model.eval()\n",
        "    #   val_preds = []\n",
        "    #   with torch.no_grad():\n",
        "    #       for essays, _ in full_val_loader:\n",
        "    #           output = model(essays).cpu().numpy()\n",
        "    #           pred = torch.sigmoid(torch.tensor(output)).numpy()\n",
        "    #           pred = np.where(pred > 0.5, 1, 0)\n",
        "    #           val_preds.extend(pred)\n",
        "\n",
        "    #   acc = accuracy_score(y_val, val_preds)\n",
        "    #   recall = recall_score(y_val, val_preds, average='macro')\n",
        "    #   precision = precision_score(y_val, val_preds, average='macro')\n",
        "    #   f1 = f1_score(y_val, val_preds, average='macro')\n",
        "    #   print(f'Accuracy: {acc}, Recall: {recall}, Precision: {precision}, F1: {f1}')\n",
        "    #   return acc\n",
        "\n",
        "    # study = optuna.create_study(direction='maximize')\n",
        "    # study.optimize(tuning_parameters, n_trials=10)\n",
        "    # print('Number of finished trials:', len(study.trials))\n",
        "    # print('Best trial:', study.best_trial.params)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    full_train_dataset = EssayDataset(X_train.tolist(), y_train, 512, vocab)\n",
        "    full_train_loader = DataLoader(full_train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "    model2 = BiLSTM_CNN(len(vocab), embedding_size, 256, embedding_matrix)\n",
        "    optimizer = torch.optim.Adam(model2.parameters(), lr = 0.00014009792265935007, weight_decay = 4.032946922784683e-06)\n",
        "    criterion  = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    for epoch in range(10):\n",
        "        model2.train()\n",
        "        total_loss = 0\n",
        "        for essays, labels in full_train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            output = model2(essays)\n",
        "            loss = criterion(output, labels.float())\n",
        "            total_loss += loss.item()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        print(f'Epoch {epoch + 1}, Loss: {total_loss / len(full_train_loader)}')\n",
        "\n",
        "\n",
        "    model2.eval()\n",
        "    test_dataset = EssayDataset(X_test.tolist(), np.zeros(len(X_test)), 512, vocab)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "    final_test_preds = []\n",
        "    with torch.no_grad():\n",
        "        for essays, _ in test_loader:\n",
        "              output = model2(essays).cpu().numpy()\n",
        "              pred = torch.sigmoid(torch.tensor(output)).numpy()\n",
        "              pred = np.where(pred > 0.5, 1, 0)\n",
        "              final_test_preds.extend(pred)\n",
        "\n",
        "    acc = accuracy_score(y_test, final_test_preds)\n",
        "    recall = recall_score(y_test, final_test_preds)\n",
        "    precision = precision_score(y_test, final_test_preds)\n",
        "    f1 = f1_score(y_test, final_test_preds)\n",
        "\n",
        "    print(f'Accuracy: {acc}, Recall: {recall}, Precision: {precision}, F1: {f1}')\n",
        "    print(classification_report(y_test, final_test_preds))\n",
        "\n",
        "\n",
        "    # qwk = cohen_kappa_score(final_test_preds, true_scores, weights = 'quadratic')\n",
        "    # print(f'\\n[Full Data Model] Test QWK: {qwk}')\n",
        "\n",
        "    # return oof_preds\n",
        "    # return oof_preds, final_test_preds, avg_test_probs, y_train_values, y_test\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07U77sBhWuAu"
      },
      "source": [
        "### Checking the run time and performance for the LSTM-based model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AZBQBMTZ-1YH"
      },
      "outputs": [],
      "source": [
        "start_time = time.time()\n",
        "lstm_model_ai(X_train, X_test, y_train, y_test)\n",
        "end_time = time.time()\n",
        "print(f'Time taken: {end_time - start_time} seconds')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bh5_ub_9W3lp"
      },
      "source": [
        "### Creating the Random Forest model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HcaTOwf43YJW"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from sklearn.base import clone\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import spacy\n",
        "from spellchecker import SpellChecker\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
        "import re\n",
        "\n",
        "def random_forest_model_with_gridsearch_ai(X_train, X_test, y_train, y_test):\n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "    spell = SpellChecker()\n",
        "\n",
        "    def cleaning_data(text):\n",
        "      text = text.replace('\\n', ' ')\n",
        "      text = text.replace('\\t', ' ')\n",
        "      text = text.replace('\\r', ' ')\n",
        "      text = re.sub(r'\\s+', ' ', text)\n",
        "      text = text.strip()\n",
        "      return text\n",
        "\n",
        "    X_train = X_train.apply(cleaning_data)\n",
        "    X_test = X_test.apply(cleaning_data)\n",
        "\n",
        "    def feature_creation(essay):\n",
        "        doc = nlp(essay)\n",
        "        num_sentences = len(list(doc.sents))\n",
        "        num_words = len(essay.split())\n",
        "        num_unique_words = len(set(essay.split()))\n",
        "        num_stop_words = len([token for token in doc if token.is_stop])\n",
        "        num_proper_nouns = len([token for token in doc if token.pos_ == 'PROPN'])\n",
        "        num_verbs = len([token for token in doc if token.pos_ == 'VERB'])\n",
        "        num_adjectives = len([token for token in doc if token.pos_ == 'ADJ'])\n",
        "        num_adverbs = len([token for token in doc if token.pos_ == 'ADV'])\n",
        "        num_nouns = len([token for token in doc if token.pos_ == 'NOUN'])\n",
        "        num_prepositions = len([token for token in doc if token.pos_ == 'ADP'])\n",
        "        num_pronouns = len([token for token in doc if token.pos_ == 'PRON'])\n",
        "        num_conjunctions = len([token for token in doc if token.pos_ == 'CCONJ'])\n",
        "        num_interjections = len([token for token in doc if token.pos_ == 'INTJ'])\n",
        "        num_punctuation = len([token for token in doc if token.pos_ == 'PUNCT'])\n",
        "        num_digits = len([token for token in doc if token.pos_ == 'NUM'])\n",
        "        num_entities = len(list(doc.ents))\n",
        "        num_spelling_errors = len(spell.unknown(essay.split()))\n",
        "        avg_word_length = np.mean([len(word) for word in essay.split()])\n",
        "        avg_sentence_length = np.mean([len(sent) for sent in list(doc.sents)])\n",
        "        return [\n",
        "            num_sentences, num_words, num_unique_words, num_stop_words, num_proper_nouns,\n",
        "            num_verbs, num_adjectives, num_adverbs, num_nouns, num_prepositions,\n",
        "            num_pronouns, num_conjunctions, num_interjections, num_punctuation, num_digits,\n",
        "            num_entities, num_spelling_errors, avg_word_length, avg_sentence_length\n",
        "        ]\n",
        "\n",
        "    features = np.array([feature_creation(essay.strip()) for essay in X_train])\n",
        "    features_test = np.array([feature_creation(essay.strip()) for essay in X_test])\n",
        "\n",
        "    # unique_scores = sorted(list(set(np.concatenate([np.array(y_train), np.array(y_test)]))))\n",
        "    # score_to_class = {score: i for i, score in enumerate(unique_scores)}\n",
        "    # class_to_score = {i: score for score, i in score_to_class.items()}\n",
        "\n",
        "    # y_train_class = y_train.map(score_to_class)\n",
        "    # y_test_class = y_test.map(score_to_class)\n",
        "\n",
        "    binned_y = pd.qcut(y_train, q=5, duplicates='drop', labels=False)\n",
        "    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "    oof_preds = np.zeros((len(y_train), 2))\n",
        "    y_train_values = np.zeros(len(y_train))\n",
        "    test_probs_folds = []\n",
        "\n",
        "    param_grid = {\n",
        "        'n_estimators': [100, 300, 500],\n",
        "        'max_depth': [None, 10, 20, 50],\n",
        "        'min_samples_split': [2, 5, 7],\n",
        "        'max_features': ['sqrt', 'log2']\n",
        "    }\n",
        "\n",
        "    grid_search = GridSearchCV(\n",
        "        RandomForestClassifier(random_state=42),\n",
        "        param_grid,\n",
        "        cv=5,\n",
        "        n_jobs=-1,\n",
        "        scoring='accuracy'\n",
        "    )\n",
        "    grid_search.fit(features, y_train)\n",
        "    print('Best Parameters:', grid_search.best_params_)\n",
        "\n",
        "    best_model = grid_search.best_estimator_\n",
        "\n",
        "\n",
        "    model2 = clone(best_model)\n",
        "    model2.fit(features, y_train)\n",
        "\n",
        "    final_test_preds = model2.predict(features_test)\n",
        "    acc = accuracy_score(y_test, final_test_preds)\n",
        "    recall = recall_score(y_test, final_test_preds)\n",
        "    precision = precision_score(y_test, final_test_preds)\n",
        "    f1 = f1_score(y_test, final_test_preds)\n",
        "    print(f'Accuracy: {acc}, Recall: {recall}, Precision: {precision}, F1: {f1}')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwmlA-zYXDjX"
      },
      "source": [
        "### Checking the run time and performance for the Random Forest model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7bJowLKNBcsO"
      },
      "outputs": [],
      "source": [
        "start_time = time.time()\n",
        "random_forest_model_with_gridsearch_ai(X_train, X_test, y_train, y_test)\n",
        "end_time = time.time()\n",
        "print(f'Time taken: {end_time - start_time} seconds')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymtkYhrMkGYQ"
      },
      "source": [
        "# Creating AI-detection stacking model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GehPi2XCYBBv"
      },
      "source": [
        "### Creating BERT-based model for stacking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "001WIHMbZSl6"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "from sklearn.metrics import cohen_kappa_score, accuracy_score, recall_score, precision_score, f1_score, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "import optuna\n",
        "from torch.nn.functional import softmax\n",
        "import re\n",
        "\n",
        "\n",
        "\n",
        "# Best Params: {'lr': 2e-05, 'batch_size': 32}\n",
        "def generate_stacking_preds_classification_ai_time(model_name, X_train, X_test, y_train, y_test, lr = 0.00001, bs = 16, epochs = 30):\n",
        "  class EssayDatasetClassification(Dataset):\n",
        "        def __init__(self, encodings, labels, seq_len = 512):\n",
        "            self.encodings = encodings\n",
        "            self.labels = labels\n",
        "            self.seq_len = 512\n",
        "\n",
        "        def __getitem__(self, item):\n",
        "            return {\n",
        "                'input_ids': self.encodings['input_ids'][item].clone().detach(),\n",
        "                'attention_mask': self.encodings['attention_mask'][item].clone().detach(),\n",
        "                'labels': torch.tensor(self.labels[item], dtype=torch.long)\n",
        "            }\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self.labels)\n",
        "\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "\n",
        "  X_train = X_train.reset_index(drop=True)\n",
        "  y_train = y_train.reset_index(drop=True)\n",
        "\n",
        "  X_test = X_test.reset_index(drop=True)\n",
        "  y_test = y_test.reset_index(drop=True)\n",
        "\n",
        "  # unique_scores = sorted(list(set(np.concatenate([np.array(y_train), np.array(y_test)]))))\n",
        "  # num_classes = len(score_to_class)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def make_compute_metrics():\n",
        "    def compute_metrics(eval_pred):\n",
        "        logits, labels = eval_pred\n",
        "        preds = np.argmax(logits, axis=-1)\n",
        "        acc = accuracy_score(labels, preds)\n",
        "        recall = recall_score(labels, preds)\n",
        "        precision = precision_score(labels, preds)\n",
        "        f1 = f1_score(labels, preds)\n",
        "        return  {'accuracy': acc,\n",
        "                 'recall': recall,\n",
        "                 'precision': precision,\n",
        "                 'F1 score': f1}\n",
        "    return compute_metrics\n",
        "\n",
        "  def tokenize_texts(text):\n",
        "    cleaned_texts = []\n",
        "    for t in text:\n",
        "      t = t.replace('\\n', ' ').replace('\\r', ' ').replace('\\t', ' ')\n",
        "      t = re.sub(r'\\s+', ' ', t)\n",
        "      t = t.strip()\n",
        "      cleaned_texts.append(t)\n",
        "\n",
        "    return tokenizer(cleaned_texts, padding='max_length', truncation=True, max_length=512, return_tensors='pt')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  binned_y = pd.qcut(y_train, q=5, duplicates='drop', labels = False)\n",
        "  kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "  preds = np.zeros((len(y_train), 2))\n",
        "  test_preds = []\n",
        "\n",
        "  for train_idx, val_idx in kf.split(X_train, binned_y):\n",
        "          model = AutoModelForSequenceClassification.from_pretrained(model_name, problem_type='single_label_classification', num_labels = 2)\n",
        "\n",
        "          train_data = X_train.iloc[train_idx].tolist()\n",
        "          val_data = X_train.iloc[val_idx].tolist()\n",
        "          train_labels = y_train.iloc[train_idx].tolist()\n",
        "          val_labels = y_train.iloc[val_idx].tolist()\n",
        "\n",
        "          train_encoding = tokenize_texts(train_data)\n",
        "          val_encoding = tokenize_texts(val_data)\n",
        "\n",
        "          test_encoding = tokenize_texts(X_test.tolist())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "          train_dataset = EssayDatasetClassification(train_encoding, train_labels)\n",
        "          test_dataset = EssayDatasetClassification(test_encoding, y_test)\n",
        "          val_dataset = EssayDatasetClassification(val_encoding, val_labels)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "          for name, param in model.named_parameters():\n",
        "              param.requires_grad = False\n",
        "\n",
        "          for name, param in model.named_parameters():\n",
        "            if any(layer in name for layer in ['encoder.layer.8', 'encoder.layer.9','encoder.layer.10', 'encoder.layer.11', 'pooler', 'classifier']):\n",
        "                param.requires_grad = True\n",
        "\n",
        "          # min_score = min(y_train)\n",
        "          # max_score = max(y_train)\n",
        "\n",
        "\n",
        "          training_args = TrainingArguments(\n",
        "            output_dir='./results',\n",
        "            learning_rate=lr,\n",
        "            per_device_train_batch_size=bs,\n",
        "            per_device_eval_batch_size=bs,\n",
        "            num_train_epochs= epochs,\n",
        "            logging_strategy = 'epoch',\n",
        "            eval_strategy = 'epoch',\n",
        "            save_strategy = 'epoch',\n",
        "            load_best_model_at_end = True,\n",
        "            metric_for_best_model = 'accuracy',\n",
        "            greater_is_better=True,\n",
        "            report_to = 'none',\n",
        "\n",
        "\n",
        "        )\n",
        "\n",
        "      #   train_dataset = EssayDatasetRegression(train_encoding, y_train)\n",
        "      #   test_dataset = EssayDatasetRegression(test_encoding, y_test)\n",
        "      #   val_dataset = EssayDatasetRegression(val_encoding, y_val)\n",
        "\n",
        "          trainer = Trainer(\n",
        "              model=model,\n",
        "              args=training_args,\n",
        "              train_dataset=train_dataset,\n",
        "              eval_dataset=val_dataset,\n",
        "              compute_metrics=make_compute_metrics(),\n",
        "              callbacks = [EarlyStoppingCallback(early_stopping_patience=3)]\n",
        "          )\n",
        "\n",
        "          trainer.train()\n",
        "\n",
        "          results = trainer.predict(val_dataset)\n",
        "          val_preds = np.argmax(results.predictions, axis=1)\n",
        "          val_probs = softmax(torch.tensor(results.predictions), dim=1).numpy()\n",
        "          preds[val_idx] = val_probs\n",
        "\n",
        "\n",
        "          results_test = trainer.predict(test_dataset)\n",
        "          test_preds_2 = np.argmax(results_test.predictions, axis=1)\n",
        "          test_probs = softmax(torch.tensor(results_test.predictions), dim=1).numpy()\n",
        "          test_preds.append(test_probs)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  avg_test_preds = np.mean(test_preds, axis=0)\n",
        "  final_test_preds_scores= np.argmax(avg_test_preds, axis=1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  return preds, avg_test_preds, y_train, y_test\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHq6W5UnYRlt"
      },
      "source": [
        "### Creating LSTM-based model for stacking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MjMqTFjHgJhM"
      },
      "outputs": [],
      "source": [
        "import gensim.downloader as api\n",
        "from gensim.models import KeyedVectors\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from collections import Counter\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, classification_report, confusion_matrix\n",
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "import optuna\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "import re\n",
        "from nltk import word_tokenize\n",
        "# Best Params: {'hidden_dim': 256, 'lr': 0.00014009792265935007, 'weight_decay': 4.032946922784683e-06, 'dropout1': 0.28928948977032815, 'dropout2': 0.3824601758543915, 'dropout3': 0.3210742837267551, 'batch_size': 32}\n",
        "def lstm_model_ai_time(X_train, X_test, y_train, y_test):\n",
        "    w2v_model = api.load('word2vec-google-news-300')\n",
        "    embedding_size = w2v_model.vector_size\n",
        "\n",
        "    X_train = X_train.reset_index(drop=True)\n",
        "    y_train = y_train.reset_index(drop=True)\n",
        "\n",
        "    X_test = X_test.reset_index(drop=True)\n",
        "    y_test = y_test.reset_index(drop=True)\n",
        "\n",
        "    # unique_scores = sorted(list(set(np.concatenate([np.array(y_train), np.array(y_test)]))))\n",
        "    # score_to_class = {score: i for i, score in enumerate(unique_scores)}\n",
        "    # class_to_score = {i: score for score, i in score_to_class.items()}\n",
        "    # num_classes = len(score_to_class)\n",
        "\n",
        "    # y_train = pd.Series([score_to_class[s] for s in y_train])\n",
        "    # y_test = pd.Series([score_to_class[s] for s in y_test])\n",
        "\n",
        "    # y_train = y_train.reset_index(drop=True)\n",
        "    # y_test = y_test.reset_index(drop=True)\n",
        "\n",
        "    # min_score = min(min(y_train), min(y_test))\n",
        "    # max_score = max(max(y_train), max(y_test))\n",
        "\n",
        "\n",
        "    class Vocab:\n",
        "        def __init__(self, token_freqs, min_freq=1, specials=['<pad>', '<unk>']):\n",
        "            self.itos = list(specials)\n",
        "            self.stoi = {tok: i for i, tok in enumerate(self.itos)}\n",
        "            for token, freq in token_freqs.items():\n",
        "                if freq >= min_freq and token not in self.stoi:\n",
        "                    self.stoi[token] = len(self.itos)\n",
        "                    self.itos.append(token)\n",
        "        def __len__(self):\n",
        "            return len(self.itos)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    class EssayDataset(Dataset):\n",
        "        def __init__(self, essays, labels, max_len, vocab):\n",
        "            self.essays = essays\n",
        "            self.labels = labels\n",
        "            self.max_length = max_len\n",
        "            self.vocab = vocab\n",
        "\n",
        "        def create_encodings(self, text):\n",
        "            word_token = word_tokenize(text.lower())\n",
        "            input_ids = [self.vocab.stoi.get(word, self.vocab.stoi['<unk>']) for word in word_token]\n",
        "            if len(input_ids) < self.max_length:\n",
        "                input_ids += [self.vocab.stoi['<pad>']] * (self.max_length - len(input_ids))\n",
        "            else:\n",
        "                input_ids = input_ids[:self.max_length]\n",
        "            return torch.tensor(input_ids)\n",
        "\n",
        "        def __getitem__(self, index):\n",
        "            essay = self.create_encodings(self.essays[index])\n",
        "            label = float(self.labels[index])\n",
        "            return essay, torch.tensor(label)\n",
        "\n",
        "        def __len__(self):\n",
        "            return len(self.essays)\n",
        "\n",
        "    class BiLSTM_CNN(nn.Module):\n",
        "        def __init__(self, vocab_size, embedding_dim, hidden_dim, embedding_matrix):\n",
        "            super().__init__()\n",
        "            self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
        "            self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
        "            self.conv1 = nn.Conv1d(hidden_dim * 2, hidden_dim, kernel_size=3, padding=1)\n",
        "            self.dropout1 = nn.Dropout(0.28928948977032815)\n",
        "            self.conv2 = nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, padding=1)\n",
        "            self.dropout2 = nn.Dropout(0.3824601758543915)\n",
        "            self.pool = nn.AdaptiveAvgPool1d(1)\n",
        "            self.dropout3 = nn.Dropout( 0.3210742837267551)\n",
        "            self.fc = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "        def forward(self, x):\n",
        "            embedded = self.embedding(x)\n",
        "            lstm_out, _ = self.lstm(embedded)\n",
        "            conv1_out = torch.relu(self.conv1(lstm_out.permute(0, 2, 1)))\n",
        "            conv2_out = torch.relu(self.conv2(conv1_out))\n",
        "            pool_out = self.pool(conv2_out).squeeze(2)\n",
        "            output = self.fc(pool_out).squeeze(1)\n",
        "            return output\n",
        "\n",
        "    def cleaning_data(text):\n",
        "        text = text.replace('\\n', ' ')\n",
        "        text = text.replace('\\t', ' ')\n",
        "        text = text.replace('\\r', ' ')\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        text = text.strip()\n",
        "        return text\n",
        "\n",
        "    essay_data = X_train.tolist() + X_test.tolist()\n",
        "    essays = [cleaning_data(essay) for essay in essay_data]\n",
        "    X_train = pd.Series(essays[:len(X_train)])\n",
        "    X_test = pd.Series(essays[len(X_train):])\n",
        "\n",
        "\n",
        "\n",
        "    counter = Counter()\n",
        "    for essay in essays:\n",
        "        counter.update(word_tokenize(essay.lower()))\n",
        "    most_common = counter.most_common(4000)\n",
        "    vocab = Vocab(dict(most_common), min_freq=1)\n",
        "\n",
        "    embedding_matrix = np.zeros((len(vocab), embedding_size))\n",
        "    for i, word in enumerate(vocab.itos):\n",
        "        embedding_matrix[i] = w2v_model[word] if word in w2v_model else np.random.normal(scale=0.6, size=(embedding_size,))\n",
        "    embedding_matrix = torch.tensor(embedding_matrix, dtype=torch.float32)\n",
        "\n",
        "    min_score = int(min(y_train.min(), y_test.min()))\n",
        "    max_score = int(max(y_train.max(), y_test.max()))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    bins = pd.qcut(y_train, q=5, duplicates='drop', labels = False)\n",
        "    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    oof_preds = np.zeros((len(y_train), 2))\n",
        "    y_train_values = np.zeros(len(y_train))\n",
        "    test_preds = []\n",
        "    # scaler = MinMaxScaler()\n",
        "    # scaler.fit(y_train.values.reshape(-1, 1)\n",
        "\n",
        "\n",
        "    for train_idx, val_idx in kf.split(X_train, bins):\n",
        "        X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
        "        y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
        "\n",
        "        train_dataset = EssayDataset(X_tr.tolist(), y_tr.tolist(), 512, vocab)\n",
        "        val_dataset = EssayDataset(X_val.tolist(), y_val.tolist(), 512, vocab)\n",
        "\n",
        "        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "        model = BiLSTM_CNN(len(vocab), embedding_size,256, embedding_matrix)\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr = 0.00014009792265935007, weight_decay = 4.032946922784683e-06)\n",
        "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer,\n",
        "        mode='max',\n",
        "        factor=0.5,\n",
        "        patience=1\n",
        "\n",
        "      )\n",
        "        criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "        for epoch in range(10):\n",
        "            model.train()\n",
        "            total_loss = 0\n",
        "            for essays, labels in train_loader:\n",
        "                optimizer.zero_grad()\n",
        "                output = model(essays)\n",
        "                loss = criterion(output, labels.float())\n",
        "                total_loss += loss.item()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "            print(f'Epoch {epoch + 1}, Loss: {total_loss / len(train_loader)}')\n",
        "\n",
        "        model.eval()\n",
        "        val_preds = []\n",
        "        val_probs = []\n",
        "        with torch.no_grad():\n",
        "            for essays, _ in val_loader:\n",
        "                output = model(essays).cpu().numpy()\n",
        "                probs = torch.sigmoid(torch.tensor(output)).numpy()\n",
        "                both_probs = np.stack([1 - probs, probs], axis=1)\n",
        "                val_probs.append(both_probs)\n",
        "                preds = np.where(probs > 0.5, 1, 0)\n",
        "                val_preds.append(preds)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        val_probs = np.concatenate(val_probs, axis=0)\n",
        "        val_preds = np.concatenate(val_preds, axis=0)\n",
        "        oof_preds[val_idx] = val_probs\n",
        "\n",
        "        y_train_values[val_idx] = y_val.tolist()\n",
        "        accuracy = accuracy_score(y_val, val_preds)\n",
        "\n",
        "        scheduler.step(accuracy)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        test_dataset = EssayDataset(X_test.tolist(), np.zeros(len(X_test)), 512, vocab)\n",
        "        test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "        model.eval()\n",
        "        fold_test_preds = []\n",
        "        fold_test_probs = []\n",
        "        with torch.no_grad():\n",
        "            for essays, _ in test_loader:\n",
        "                output = model(essays).cpu().numpy()\n",
        "                probs = torch.sigmoid(torch.tensor(output)).numpy()\n",
        "                both_probs = np.stack([1 - probs, probs], axis=1)\n",
        "                fold_test_probs.append(both_probs)\n",
        "                preds = np.where(probs > 0.5, 1, 0)\n",
        "                fold_test_preds.append(preds)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        test_preds.append(np.concatenate(fold_test_probs, axis=0))\n",
        "\n",
        "\n",
        "    avg_test_probs = np.mean(test_preds, axis=0)\n",
        "    final_test_preds = np.argmax(avg_test_probs, axis=1)\n",
        "\n",
        "    labels = y_test.tolist()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    return oof_preds, avg_test_probs, y_train_values, y_test\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgJNKZMeYfkP"
      },
      "source": [
        "### Create Random Forest Model for stacking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XyT5J3MYzkH4"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from sklearn.base import clone\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import spacy\n",
        "from spellchecker import SpellChecker\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
        "import re\n",
        "\n",
        "def random_forest_model_with_gridsearch_ai_time(X_train, X_test, y_train, y_test):\n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "    spell = SpellChecker()\n",
        "\n",
        "    def cleaning_data(text):\n",
        "      text = text.replace('\\n', ' ')\n",
        "      text = text.replace('\\t', ' ')\n",
        "      text = text.replace('\\r', ' ')\n",
        "      text = re.sub(r'\\s+', ' ', text)\n",
        "      text = text.strip()\n",
        "      return text\n",
        "\n",
        "    X_train = X_train.apply(cleaning_data)\n",
        "    X_test = X_test.apply(cleaning_data)\n",
        "\n",
        "    def feature_creation(essay):\n",
        "        doc = nlp(essay)\n",
        "        num_sentences = len(list(doc.sents))\n",
        "        num_words = len(essay.split())\n",
        "        num_unique_words = len(set(essay.split()))\n",
        "        num_stop_words = len([token for token in doc if token.is_stop])\n",
        "        num_proper_nouns = len([token for token in doc if token.pos_ == 'PROPN'])\n",
        "        num_verbs = len([token for token in doc if token.pos_ == 'VERB'])\n",
        "        num_adjectives = len([token for token in doc if token.pos_ == 'ADJ'])\n",
        "        num_adverbs = len([token for token in doc if token.pos_ == 'ADV'])\n",
        "        num_nouns = len([token for token in doc if token.pos_ == 'NOUN'])\n",
        "        num_prepositions = len([token for token in doc if token.pos_ == 'ADP'])\n",
        "        num_pronouns = len([token for token in doc if token.pos_ == 'PRON'])\n",
        "        num_conjunctions = len([token for token in doc if token.pos_ == 'CCONJ'])\n",
        "        num_interjections = len([token for token in doc if token.pos_ == 'INTJ'])\n",
        "        num_punctuation = len([token for token in doc if token.pos_ == 'PUNCT'])\n",
        "        num_digits = len([token for token in doc if token.pos_ == 'NUM'])\n",
        "        num_entities = len(list(doc.ents))\n",
        "        num_spelling_errors = len(spell.unknown(essay.split()))\n",
        "        avg_word_length = np.mean([len(word) for word in essay.split()])\n",
        "        avg_sentence_length = np.mean([len(sent) for sent in list(doc.sents)])\n",
        "        return [\n",
        "            num_sentences, num_words, num_unique_words, num_stop_words, num_proper_nouns,\n",
        "            num_verbs, num_adjectives, num_adverbs, num_nouns, num_prepositions,\n",
        "            num_pronouns, num_conjunctions, num_interjections, num_punctuation, num_digits,\n",
        "            num_entities, num_spelling_errors, avg_word_length, avg_sentence_length\n",
        "        ]\n",
        "\n",
        "    features = np.array([feature_creation(essay.strip()) for essay in X_train])\n",
        "    features_test = np.array([feature_creation(essay.strip()) for essay in X_test])\n",
        "\n",
        "    # unique_scores = sorted(list(set(np.concatenate([np.array(y_train), np.array(y_test)]))))\n",
        "    # score_to_class = {score: i for i, score in enumerate(unique_scores)}\n",
        "    # class_to_score = {i: score for score, i in score_to_class.items()}\n",
        "\n",
        "    # y_train_class = y_train.map(score_to_class)\n",
        "    # y_test_class = y_test.map(score_to_class)\n",
        "\n",
        "    binned_y = pd.qcut(y_train, q=5, duplicates='drop', labels=False)\n",
        "    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "    oof_preds = np.zeros((len(y_train), 2))\n",
        "    y_train_values = np.zeros(len(y_train))\n",
        "    test_probs_folds = []\n",
        "\n",
        "    param_grid = {\n",
        "        'n_estimators': [100, 300, 500],\n",
        "        'max_depth': [None, 10, 20, 50],\n",
        "        'min_samples_split': [2, 5, 7],\n",
        "        'max_features': ['sqrt', 'log2']\n",
        "    }\n",
        "\n",
        "    grid_search = GridSearchCV(\n",
        "        RandomForestClassifier(random_state=42),\n",
        "        param_grid,\n",
        "        cv=5,\n",
        "        n_jobs=-1,\n",
        "        scoring='accuracy'\n",
        "    )\n",
        "    grid_search.fit(features, y_train)\n",
        "    print('Best Parameters:', grid_search.best_params_)\n",
        "\n",
        "    best_model = grid_search.best_estimator_\n",
        "\n",
        "    for train_idx, val_idx in kf.split(X_train, binned_y):\n",
        "        X_tr, X_val = features[train_idx], features[val_idx]\n",
        "        y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
        "\n",
        "        model = clone(best_model)\n",
        "        model.fit(X_tr, y_tr)\n",
        "\n",
        "        val_probs = model.predict_proba(X_val)\n",
        "        val_preds = np.argmax(val_probs, axis=1)\n",
        "\n",
        "        oof_preds[val_idx] = val_probs\n",
        "        y_train_values[val_idx] = y_val\n",
        "\n",
        "\n",
        "\n",
        "        test_probs_folds.append(model.predict_proba(features_test))\n",
        "\n",
        "\n",
        "    avg_test_probs = np.mean(test_probs_folds, axis=0)\n",
        "    final_test_class_preds = np.argmax(avg_test_probs, axis=1)\n",
        "\n",
        "\n",
        "    return oof_preds, avg_test_probs, y_train_values, y_test.tolist()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEBgllA7Ys4h"
      },
      "source": [
        "### Creating Stacking model for AI detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U2VtsSl22qyR"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from sklearn.linear_model import RidgeCV, LogisticRegressionCV\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import optuna\n",
        "\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "\n",
        "\n",
        "oof_preds_transformer, preds_test_k_fold_avg_bert, bert_y_train, bert_y_test = generate_stacking_preds_classification_ai_time('bert-base-uncased', X_train, X_test, y_train, y_test, lr = 0.00002, bs = 32, epochs = 30)\n",
        "bert_completion_time = time.time()\n",
        "print(f'BERT took {bert_completion_time - start_time} seconds')\n",
        "\n",
        "\n",
        "oof_preds_lstm, preds_test_k_fold_avg_lstm, lstm_y_train, lstm_y_test =                                        lstm_model_ai_time(X_train, X_test, y_train, y_test)\n",
        "lstm_completion_time = time.time()\n",
        "print(f'LSTM took {lstm_completion_time - bert_completion_time} seconds')\n",
        "\n",
        "oof_preds_rf, preds_test_k_fold_avg_rf, y_train_rf, y_test_rf = random_forest_model_with_gridsearch_ai_time(X_train, X_test, y_train, y_test)\n",
        "rf_completion_time = time.time()\n",
        "print(f'RF took {rf_completion_time - lstm_completion_time} seconds')\n",
        "\n",
        "from sklearn.linear_model import RidgeCV\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "\n",
        "\n",
        "X_meta_train = np.concatenate([oof_preds_transformer, oof_preds_lstm, oof_preds_rf], axis=1)\n",
        "X_meta_test = np.concatenate([preds_test_k_fold_avg_bert, preds_test_k_fold_avg_lstm, preds_test_k_fold_avg_rf], axis=1)\n",
        "\n",
        "\n",
        "meta_model = LogisticRegressionCV(max_iter=10000, cv=5)\n",
        "meta_model.fit(X_meta_train, y_train)\n",
        "meta_preds = meta_model.predict(X_meta_test)\n",
        "\n",
        "acc = accuracy_score(y_test, meta_preds)\n",
        "recall = recall_score(y_test, meta_preds)\n",
        "precision = precision_score(y_test, meta_preds)\n",
        "f1 = f1_score(y_test, meta_preds)\n",
        "print(f'Accuracy:  {acc}')\n",
        "print(f'Recall:    {recall}')\n",
        "print(f'Precision: {precision}')\n",
        "print(f'F1 Score:  {f1}')\n",
        "\n",
        "print(classification_report(y_test, meta_preds, digits=4))\n",
        "print(confusion_matrix(y_test, meta_preds))\n",
        "\n",
        "\n",
        "\n",
        "stacking_completion_time = time.time()\n",
        "print(f'Stacking took {stacking_completion_time - start_time} seconds')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZ7NtNr6Y4sS"
      },
      "source": [
        "### Creating correlation matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zSWpoJcm0wbY"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "preds_df = pd.DataFrame({\n",
        "    'BERT': preds_test_k_fold_avg_bert.flatten(),\n",
        "    'LSTM': preds_test_k_fold_avg_lstm.flatten(),\n",
        "    'RF': preds_test_k_fold_avg_rf.flatten(),\n",
        "})\n",
        "\n",
        "\n",
        "correlation_matrix = preds_df.corr(method='pearson')\n",
        "print('Correlation matrix:')\n",
        "print(correlation_matrix)\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=0.0, vmax=1.0)\n",
        "plt.title('Correlation of Base Model Predictions')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KPJpAsZY_Is"
      },
      "source": [
        "### Modelling training times"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_h4uMXS_-t2"
      },
      "outputs": [],
      "source": [
        "box_plot_data_aes = pd.DataFrame({\n",
        "    'BERT': [171.82, 146.08, 122.56, 200.03, 189.84, 168, 183.2, 267.5],\n",
        "    'LSTM' : [352.24, 201.99, 119.01, 319.89, 322.78, 319.29, 170.87, 175.63],\n",
        "    'RF': [146.44, 151.03, 66.8, 65.22, 77.74, 85.88, 81.4, 95.51],\n",
        "    'Stacking Model 1': [2069.53, 2086.37, 1046.4, 1822.57, 2003.22, 2075.3, 1947.63, 2100.08],\n",
        "    'Stacking Model 2': [2145.5, 2124.99, 1125.88, 2007.73, 2138.44, 2058.07, 1866.04, 2110.49]\n",
        "})\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "box_plot_data_aes.boxplot(vert=False)\n",
        "plt.xticks(rotation=45)\n",
        "plt.ylabel('Model Type', size=15)\n",
        "plt.xlabel('Time taken to train (seconds)', size=15, labelpad = 10)\n",
        "plt.title('Time taken to train models for an AES system', size = 15)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "bar_graph_ai = pd.Series({\n",
        "    'BERT' : 497.92,\n",
        "    'LSTM' : 1751.66,\n",
        "    'RF' : 844.09,\n",
        "    'Stacking' : 11415.63\n",
        "})\n",
        "\n",
        "plt.figsize=(12, 6)\n",
        "plt.bar(bar_graph_ai.index, bar_graph_ai.values)\n",
        "plt.xticks(rotation=45)\n",
        "plt.xlabel('Model Type', size=12)\n",
        "plt.ylabel('Time taken to train (seconds)', size=12)\n",
        "plt.title('Time Taken to train models for an AI-detection system', size = 12)\n",
        "# plt.xlabel('Model Type', size=15)\n",
        "# plt.ylabel('Time taken to train', size=15, labelpad = 10)\n",
        "# plt.title('Bar Graph of Time Taken to Train Models for an AES system', size = 15)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qx-Ju8qrZDTO"
      },
      "source": [
        "### Creating a confusion matrix from results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2M2xiaNxosho"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "cm = np.array([[999,1],\n",
        "              [12,988]])\n",
        "\n",
        "labels = ['Negative', 'Positive']\n",
        "df_cm = pd.DataFrame(cm, index = labels, columns = labels)\n",
        "plt.figure(figsize = (10,7))\n",
        "sns.heatmap(df_cm, annot=True, fmt='g', cmap='Blues')\n",
        "plt.title('Confusion Matrix for AI-detection system', size=20)\n",
        "plt.xlabel('Predicted Labels', labelpad=20, size=15)\n",
        "plt.ylabel('True Labels', labelpad=20, size=15)\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}